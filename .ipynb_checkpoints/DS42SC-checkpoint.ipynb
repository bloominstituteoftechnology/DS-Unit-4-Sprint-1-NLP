{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(whitespace_string.split()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'March 8, 2015\\nMarch 15, 2015\\nMarch 22, 2015\\nMarch 29, 2015\\nApril 5, 2015\\nApril 12, 2015\\nApril 19, 2015\\nApril 26, 2015\\nMay 3, 2015\\nMay 10, 2015\\nMay 17, 2015\\nMay 24, 2015\\nMay 31, 2015\\nJune 7, 2015\\nJune 14, 2015\\nJune 21, 2015\\nJune 28, 2015\\nJuly 5, 2015\\nJuly 12, 2015\\nJuly 19, 2015'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dates.txt', 'r', encoding='utf-8') as f:\n",
    "  contents = f.read()\n",
    "  \n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('March', '8', '2015')\n",
      "('March', '15', '2015')\n",
      "('March', '22', '2015')\n",
      "('March', '29', '2015')\n",
      "('April', '5', '2015')\n",
      "('April', '12', '2015')\n",
      "('April', '19', '2015')\n",
      "('April', '26', '2015')\n",
      "('May', '3', '2015')\n",
      "('May', '10', '2015')\n",
      "('May', '17', '2015')\n",
      "('May', '24', '2015')\n",
      "('May', '31', '2015')\n",
      "('June', '7', '2015')\n",
      "('June', '14', '2015')\n",
      "('June', '21', '2015')\n",
      "('June', '28', '2015')\n",
      "('July', '5', '2015')\n",
      "('July', '12', '2015')\n",
      "('July', '19', '2015')\n"
     ]
    }
   ],
   "source": [
    "regex = r\"([a-zA-Z]+) (\\d+), (\\d\\d\\d\\d)\" \n",
    "\n",
    "search_result = re.findall(regex, contents)\n",
    "\n",
    "for match in search_result:\n",
    "  print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month Day  Year\n",
       "0   March   8  2015\n",
       "1   March  15  2015\n",
       "2   March  22  2015\n",
       "3   March  29  2015\n",
       "4   April   5  2015\n",
       "5   April  12  2015\n",
       "6   April  19  2015\n",
       "7   April  26  2015\n",
       "8     May   3  2015\n",
       "9     May  10  2015\n",
       "10    May  17  2015\n",
       "11    May  24  2015\n",
       "12    May  31  2015\n",
       "13   June   7  2015\n",
       "14   June  14  2015\n",
       "15   June  21  2015\n",
       "16   June  28  2015\n",
       "17   July   5  2015\n",
       "18   July  12  2015\n",
       "19   July  19  2015"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDate = pd.DataFrame(search_result, columns=['Month', 'Day','Year'])\n",
    "dfDate.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [],
   "source": [
    "#  \n",
    "url='https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv'\n",
    "df=pd.read_csv(url)\n",
    "df.head()\n",
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText'] = df['SentimentText'].str.lower()\n",
    "df.head()\n",
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. omgaga. im sooo  im gunna cry. i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my apl frie...\n",
       "1          0                     i missed the new moon trail...\n",
       "2          1                            omg its already 7:30 :o\n",
       "3          0            .. omgaga. im sooo  im gunna cry. i'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df2.copy()\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\t#tokens = str(doc).split(' ')\n",
    "    tokens = word_tokenize(doc)\n",
    "    #  lowercase\n",
    "    lowercase_tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each token\n",
    "    no_punctuation = [w.translate(table) for w in lowercase_tokens]\n",
    "    # Remove words that aren't alphabetic\n",
    "    alphabetic = [word for word in no_punctuation if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in alphabetic if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\t#tokens = [word for word in words if len(word)>1]\n",
    "    return words\n",
    "\n",
    "df['SentimentText'] = df['SentimentText'].apply(clean_doc)\n",
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText'] = df.SentimentText.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                                 [sad, apl, friend]\n",
       "1          0                       [missed, new, moon, trailer]\n",
       "2          1                                     [omg, already]\n",
       "3          0  [omgaga, im, sooo, im, gunna, cry, dentist, si...\n",
       "4          0                      [think, mi, bf, cheating, tt]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "#### Term Frequency / Inverse Document Frequency  #####\n",
    "\n",
    "Term Frequency: Percentage of words in document for each word\n",
    "  TF = num of times word occurs (frequency) divided by the total num of word in   the document\n",
    "\n",
    "Inverse Document Frequency: A penalty for the word existing in a high number of documents.\n",
    "    IDF = log2(total-#-documents/ #-documents-including-word )\n",
    "\n",
    "TF-IDF weighs a keyword in any content and assigns the importance to that keyword based on the number of times it appears in the document. It also checks how relevant the keyword is. Each word has its respective TF and IDF score. The product of the TF and IDF scores of a term is called the TF-IDF weight of that term.\n",
    "\n",
    "Put simply, the higher the TF-IDF score (weight), the rarer the term and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.SentimentText.tolist()\n",
    "y = df.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ca': 198, 'nt': 908, 'sleep': 1159, 'watching': 1407, 'quot': 1030, 'girlfriend': 531, 'experience': 425, 'school': 1100, 'haha': 570, 'case': 211, 'forgot': 500, 'la': 725, 'love': 795, 'country': 291, 'music': 879, 'little': 768, 'cake': 199, 'got': 551, 'ta': 1253, 'dont': 363, 'know': 721, 'coffee': 260, 'lt': 801, 'gt': 560, 'everybody': 413, 'dancing': 318, 'pop': 992, 'twitpic': 1352, 'working': 1453, 'photo': 962, 'wan': 1397, 'na': 883, 'hear': 595, 'bet': 127, 'cute': 310, 'like': 755, 'voice': 1388, 'friend': 510, 'told': 1308, 'youtube': 1492, 'video': 1383, 'download': 368, 'finished': 478, 'noticed': 907, 'replied': 1056, 'week': 1417, 'ago': 19, 'http': 640, 'time': 1301, 'long': 782, 'failed': 436, 'party': 947, 'dinner': 353, 'kinda': 713, 'thing': 1283, 'sign': 1142, 'good': 544, 'night': 896, 'sun': 1239, 'way': 1409, 'terrible': 1273, 'address': 10, 'sniff': 1170, 'sunny': 1241, 'today': 1307, 'yes': 1487, 'sigh': 1141, 'cool': 284, 'things': 1284, 'happening': 582, 'heyy': 613, 'hot': 633, 'day': 327, 'event': 412, 'fly': 487, 'right': 1067, 'lol': 778, 'new': 892, 'nice': 894, 'meet': 830, 'happy': 584, 'mother': 869, 'sad': 1083, 'step': 1215, 'come': 266, 'mommy': 861, 'makes': 813, 'mad': 808, 'yeah': 1482, 'remember': 1053, 'hoping': 630, 'fell': 464, 'amy': 50, 'chat': 225, 'xxx': 1475, 'yep': 1486, 'think': 1285, 'old': 922, 'far': 448, 'eh': 394, 'exams': 418, 'im': 658, 'leaving': 744, 'group': 558, 'car': 208, 'club': 257, 'traffic': 1322, 'drinking': 375, 'wine': 1433, 'buying': 196, 'iphone': 673, 'ahhh': 27, 'really': 1044, 'want': 1398, 'pictures': 968, 'badly': 106, 'shows': 1138, 'bad': 105, 'ones': 924, 'rip': 1069, 'miss': 852, 'favorite': 455, 'teacher': 1266, 'important': 662, 'person': 959, 'wo': 1439, 'help': 607, 'lose': 789, 'following': 496, 'amp': 49, 'woot': 1448, 'tummy': 1338, 'hurts': 649, 'goodnight': 546, 'twitter': 1353, 'looks': 788, 'gon': 542, 'wont': 1446, 'let': 749, 'bc': 115, 'original': 934, 'thinking': 1286, 'homework': 626, 'oh': 917, 'son': 1174, 'realize': 1042, 'congrats': 278, 'graduation': 554, 'sweetie': 1252, 'loved': 796, 'high': 615, 'ok': 920, 'beautiful': 120, 'woman': 1441, 'rock': 1072, 'thanks': 1278, 'sharing': 1124, 'xo': 1472, 'said': 1086, 'blue': 151, 'work': 1451, 'goin': 540, 'amberbenson': 46, 'afford': 14, 'year': 1483, 'gunna': 563, 'read': 1038, 'talk': 1258, 'tomorrow': 1310, 'awww': 97, 'poor': 991, 'hope': 628, 'feel': 459, 'better': 128, 'soon': 1178, 'used': 1375, 'phone': 961, 'cell': 218, 'met': 840, 'pleasure': 983, 'hi': 614, 'huge': 643, 'fan': 445, 'amazing': 45, 'make': 812, 'going': 541, 'fantastic': 447, 'start': 1205, 'sunday': 1240, 'king': 714, 'gone': 543, 'probably': 1009, 'expect': 423, 'follow': 492, 'feels': 462, 'left': 745, 'lonely': 781, 'getting': 527, 'hang': 578, 'song': 1175, 'favourite': 456, 'crazy': 299, 'bit': 138, 'weird': 1422, 'different': 351, 'men': 834, 'sell': 1109, 'hair': 574, 'use': 1374, 'need': 888, 'free': 506, 'great': 556, 'looking': 787, 'fixed': 482, 'grand': 555, 'appreciate': 64, 'rain': 1033, 'wanted': 1399, 'ya': 1476, 'morning': 868, 'wednesday': 1416, 'food': 497, 'french': 507, 'ended': 398, 'walk': 1394, 'end': 397, 'send': 1110, 'pm': 987, 'ask': 76, 'people': 956, 'invite': 671, 'yum': 1497, 'worth': 1461, 'dude': 382, 'mr': 875, 'sir': 1150, 'coming': 268, 'waiting': 1392, 'post': 995, 'wait': 1391, 'longer': 783, 'doin': 362, 'xbox': 1470, 'play': 980, 'finally': 472, 'asleep': 79, 'wow': 1462, 'speak': 1193, 'truth': 1334, 'dear': 332, 'thank': 1277, 'welcome': 1423, 'fix': 481, 'face': 432, 'whats': 1427, 'ur': 1373, 'buddy': 187, 'slept': 1162, 'father': 451, 'eat': 392, 'birthday': 137, 'followers': 494, 'thats': 1281, 'life': 753, 'plan': 975, 'nights': 897, 'awe': 93, 'able': 0, 'big': 131, 'house': 637, 'loads': 776, 'fun': 516, 'officially': 916, 'dress': 373, 'singing': 1148, 'moment': 860, 'white': 1428, 'usually': 1378, 'prefer': 1003, 'alexalltimelow': 36, 'stuff': 1232, 'sorry': 1183, 'lovely': 797, 'job': 688, 'site': 1154, 'look': 784, 'seriously': 1115, 'hours': 636, 'friday': 509, 'fair': 437, 'sore': 1182, 'feet': 463, 'lady': 728, 'tired': 1305, 'playing': 982, 'sister': 1152, 'hahahaha': 573, 'understand': 1363, 'trying': 1336, 'followfriday': 495, 'guys': 566, 'girl': 530, 'gave': 522, 'hahaha': 572, 'hugs': 644, 'wish': 1435, 'fail': 435, 'worry': 1458, 'friends': 511, 'aww': 96, 'nervous': 891, 'andyclemmensen': 53, 'close': 253, 'eyes': 429, 'talking': 1260, 'interested': 667, 'later': 734, 'tonight': 1311, 'smile': 1169, 'care': 210, 'tell': 1271, 'game': 519, 'sa': 1082, 'stupid': 1233, 'boys': 167, 'pic': 964, 'home': 625, 'july': 696, 'bout': 163, 'buy': 195, 'ticket': 1297, 'baby': 101, 'seen': 1107, 'movie': 872, 'yall': 1478, 'lunch': 804, 'sounds': 1187, 'ashleytisdale': 75, 'pa': 938, 'bring': 175, 'soup': 1188, 'add': 8, 'milk': 845, 'slightly': 1163, 'unfortunately': 1364, 'local': 777, 'size': 1157, 'order': 933, 'spend': 1196, 'scared': 1097, 'uni': 1365, 'fans': 446, 'came': 204, 'sooo': 1179, 'jealous': 686, 'times': 1302, 'clean': 250, 'early': 388, 'bday': 116, 'check': 227, 'ass': 80, 'forward': 502, 'stand': 1203, 'link': 763, 'thought': 1290, 'lie': 752, 'yea': 1481, 'instead': 666, 'musicmonday': 880, 'nope': 901, 'project': 1014, 'stuck': 1229, 'knows': 724, 'bed': 122, 'lookin': 786, 'reason': 1045, 'awake': 90, 'atm': 83, 'omg': 923, 'saw': 1093, 'ha': 568, 'certainly': 219, 'myweakness': 882, 'awesome': 94, 'damn': 316, 'monday': 862, 'cause': 216, 'freaking': 505, 'actually': 6, 'aplusk': 60, 'say': 1094, 'thoughts': 1291, 'id': 652, 'mcfly': 825, 'reply': 1058, 'cheers': 231, 'lot': 793, 'bloody': 150, 'keeping': 701, 'till': 1300, 'band': 109, 'meeting': 831, 'cheer': 230, 'hey': 612, 'chuck': 245, 'bb': 113, 'england': 399, 'real': 1041, 'sing': 1147, 'lost': 792, 'camera': 205, 'pics': 966, 'tried': 1329, 'luck': 802, 'fresh': 508, 'da': 312, 'called': 201, 'chocolate': 240, 'works': 1454, 'known': 723, 'crappy': 298, 'middle': 841, 'proper': 1016, 'stop': 1218, 'lame': 730, 'coz': 296, 'drunk': 381, 'visit': 1387, 'fall': 440, 'rest': 1061, 'took': 1312, 'social': 1171, 'media': 829, 'didnt': 346, 'brother': 180, 'dad': 313, 'likes': 758, 'nick': 895, 'says': 1096, 'head': 592, 'sure': 1248, 'away': 92, 'anymore': 57, 'hard': 585, 'ppl': 1000, 'watch': 1405, 'red': 1049, 'dat': 323, 'updated': 1368, 'peeps': 955, 'aw': 89, 'seeing': 1106, 'brain': 169, 'best': 126, 'breakfast': 172, 'open': 930, 'window': 1431, 'radio': 1032, 'iremember': 676, 'yesterday': 1488, 'twilight': 1350, 'lucky': 803, 'ran': 1036, 'moon': 867, 'prob': 1008, 'bbq': 114, 'means': 827, 'bought': 162, 'alot': 39, 'shirt': 1127, 'weeks': 1420, 'weekend': 1418, 'guitar': 562, 'join': 691, 'xd': 1471, 'past': 950, 'dang': 319, 'yay': 1480, 'hate': 587, 'spot': 1198, 'blast': 144, 'brought': 182, 'definitely': 338, 'crap': 297, 'season': 1103, 'went': 1424, 'funny': 517, 'sat': 1090, 'london': 780, 'ff': 466, 'ohh': 918, 'meant': 828, 'message': 838, 'computer': 275, 'update': 1367, 'dog': 360, 'road': 1071, 'god': 538, 'awful': 95, 'ugh': 1357, 'dunno': 384, 'broke': 178, 'lived': 770, 'ps': 1018, 'guy': 565, 'sent': 1113, 'super': 1243, 'page': 939, 'cat': 212, 'shoot': 1130, 'jokes': 693, 'keeps': 702, 'hurt': 648, 'giving': 535, 'crc': 300, 'huh': 645, 'leave': 743, 'managed': 818, 'air': 30, 'small': 1165, 'glass': 537, 'ohhh': 919, 'dropped': 380, 'days': 328, 'wear': 1410, 'nite': 898, 'evening': 411, 'dm': 357, 'info': 664, 'hehe': 602, 'confused': 277, 'especially': 409, 'apple': 63, 'die': 347, 'disappointed': 355, 'kidding': 707, 'usual': 1377, 'shoes': 1129, 'clothes': 256, 'guess': 561, 'save': 1092, 'knew': 719, 'version': 1381, 'mo': 857, 'cold': 261, 'email': 396, 'shut': 1139, 'stay': 1212, 'money': 863, 'live': 769, 'maybe': 824, 'tht': 1294, 'listening': 767, 'sex': 1119, 'hospital': 632, 'wasnt': 1402, 'blog': 148, 'comment': 269, 'fucking': 514, 'tweeting': 1347, 'felt': 465, 'happened': 581, 'tv': 1342, 'fine': 475, 'kill': 709, 'glad': 536, 'try': 1335, 'unless': 1366, 'special': 1195, 'second': 1104, 'mate': 822, 'story': 1223, 'web': 1413, 'listen': 766, 'alex': 35, 'boo': 155, 'hug': 642, 'kiss': 715, 'hanging': 579, 'hmmm': 620, 'missed': 853, 'bored': 158, 'drink': 374, 'checking': 229, 'google': 548, 'rocks': 1073, 'shit': 1128, 'safe': 1085, 'trip': 1330, 'ready': 1040, 'run': 1080, 'summer': 1238, 'sky': 1158, 'goodbye': 545, 'rite': 1070, 'sweet': 1251, 'dreams': 372, 'dc': 329, 'yummy': 1498, 'btw': 185, 'totally': 1316, 'adam': 7, 'tour': 1319, 'videos': 1384, 'break': 171, 'ice': 651, 'heart': 598, 'church': 246, 'bradiewebbstack': 168, 'fight': 467, 'forget': 499, 'alright': 40, 'feeling': 461, 'minute': 850, 'ate': 81, 'miles': 843, 'drive': 377, 'lmfao': 774, 'blame': 143, 'family': 443, 'xx': 1474, 'response': 1060, 'dead': 330, 'pick': 965, 'mail': 810, 'tweet': 1344, 'dying': 386, 'horrible': 631, 'liked': 756, 'absolutely': 1, 'started': 1206, 'using': 1376, 'lines': 762, 'forever': 498, 'lets': 750, 'dnt': 358, 'babe': 99, 'kid': 706, 'type': 1356, 'write': 1463, 'cousin': 294, 'fav': 453, 'peace': 954, 'twittering': 1354, 'congratulations': 279, 'heard': 596, 'noooo': 900, 'orange': 932, 'book': 156, 'hmm': 619, 'question': 1023, 'yo': 1489, 'picture': 967, 'ugly': 1358, 'needs': 890, 'enjoyed': 402, 'reading': 1039, 'team': 1267, 'jb': 685, 'soo': 1177, 'suppose': 1246, 'dvd': 385, 'release': 1051, 'david': 326, 'sis': 1151, 'couple': 292, 'winter': 1434, 'pain': 941, 'idea': 653, 'pissed': 971, 'happen': 580, 'plz': 986, 'eye': 428, 'comes': 267, 'loving': 799, 'hahah': 571, 'word': 1449, 'world': 1455, 'hour': 635, 'age': 17, 'loves': 798, 'havent': 590, 'america': 47, 'plus': 985, 'currently': 308, 'hello': 606, 'pay': 951, 'decided': 335, 'photos': 963, 'trending': 1328, 'topic': 1314, 'lots': 794, 'ipod': 674, 'ftw': 512, 'buckhollywood': 186, 'room': 1076, 'staying': 1214, 'turned': 1340, 'laptop': 731, 'headache': 593, 'weather': 1412, 'lolz': 779, 'nyc': 911, 'half': 575, 'memory': 833, 'theres': 1282, 'alive': 37, 'attention': 84, 'believe': 124, 'ive': 682, 'month': 864, 'um': 1361, 'amandapalmer': 44, 'appreciated': 65, 'sound': 1186, 'airport': 31, 'falling': 441, 'delicious': 339, 'died': 348, 'worst': 1460, 'pretty': 1005, 'easy': 391, 'scary': 1098, 'movies': 873, 'short': 1133, 'profile': 1013, 'comments': 270, 'gettin': 526, 'company': 272, 'wit': 1438, 'chance': 220, 'living': 772, 'mama': 816, 'mind': 848, 'cover': 295, 'sucks': 1236, 'planning': 978, 'alyankovic': 41, 'website': 1414, 'words': 1450, 'ashley': 73, 'cuz': 311, 'soooo': 1180, 'excited': 420, 'tweets': 1348, 'sick': 1140, 'husband': 650, 'daily': 315, 'fast': 449, 'raining': 1034, 'body': 154, 'quick': 1025, 'code': 259, 'hell': 604, 'deserve': 343, 'signed': 1143, 'pizza': 972, 'park': 946, 'updates': 1369, 'win': 1430, 'hav': 589, 'ty': 1355, 'wishes': 1436, 'helped': 608, 'wondering': 1445, 'years': 1484, 'learn': 740, 'wtf': 1467, 'worried': 1456, 'nap': 885, 'course': 293, 'def': 337, 'practice': 1001, 'power': 999, 'florida': 485, 'women': 1442, 'wet': 1426, 'hotel': 634, 'round': 1078, 'dream': 371, 'change': 221, 'concert': 276, 'darling': 321, 'bless': 145, 'sexy': 1120, 'wall': 1396, 'europe': 410, 'green': 557, 'places': 974, 'list': 765, 'fish': 479, 'mall': 815, 'coollike': 285, 'magic': 809, 'tough': 1318, 'note': 905, 'self': 1108, 'pull': 1020, 'twice': 1349, 'gym': 567, 'depends': 340, 'near': 886, 'closed': 254, 'crying': 305, 'shame': 1122, 'france': 503, 'flight': 483, 'fully': 515, 'babygirlparis': 102, 'joe': 689, 'kno': 720, 'dance': 317, 'goes': 539, 'broken': 179, 'stopped': 1219, 'machine': 807, 'bye': 197, 'tea': 1264, 'awwww': 98, 'wrong': 1465, 'upload': 1371, 'error': 408, 'mom': 859, 'cut': 309, 'running': 1081, 'late': 732, 'shot': 1134, 'bro': 177, 'trouble': 1331, 'nearly': 887, 'apparently': 62, 'showing': 1137, 'pass': 948, 'light': 754, 'tears': 1269, 'plans': 979, 'fuck': 513, 'wwwtweeterfollowcom': 1469, 'train': 1324, 'vip': 1386, 'silly': 1144, 'shaundiviney': 1125, 'squarespace': 1200, 'worse': 1459, 'wonder': 1443, 'gives': 534, 'mouth': 870, 'colorblindfish': 265, 'mean': 826, 'inaperfectworld': 663, 'mix': 855, 'children': 236, 'gay': 523, 'songs': 1176, 'min': 847, 'realized': 1043, 'kind': 712, 'mood': 866, 'excuse': 422, 'minutes': 851, 'suck': 1235, 'iranelection': 675, 'store': 1220, 'holy': 624, 'making': 814, 'slow': 1164, 'connection': 280, 'dogs': 361, 'teeth': 1270, 'darn': 322, 'telling': 1272, 'article': 71, 'caught': 215, 'stomach': 1217, 'flu': 486, 'gets': 525, 'australia': 87, 'til': 1299, 'thx': 1296, 'quite': 1029, 'mess': 837, 'aubreyoday': 85, 'easier': 390, 'office': 915, 'saturday': 1091, 'box': 164, 'supposed': 1247, 'wedding': 1415, 'arent': 68, 'swear': 1250, 'myspace': 881, 'character': 224, 'quickly': 1026, 'copy': 286, 'low': 800, 'andy': 52, 'daddy': 314, 'lil': 759, 'black': 140, 'paid': 940, 'problem': 1010, 'area': 67, 'woke': 1440, 'looked': 785, 'replies': 1057, 'shall': 1121, 'released': 1052, 'catch': 213, 'boston': 161, 'star': 1204, 'trek': 1327, 'point': 988, 'deep': 336, 'cost': 288, 'mum': 878, 'andrew': 51, 'adorable': 12, 'anyways': 59, 'contest': 281, 'windows': 1432, 'sleeping': 1160, 'track': 1321, 'card': 209, 'bobbyllew': 153, 'enjoying': 403, 'talented': 1257, 'episode': 406, 'likely': 757, 'bus': 192, 'difficult': 352, 'million': 846, 'chrisdjmoyles': 244, 'turn': 1339, 'puppy': 1021, 'state': 1209, 'sold': 1172, 'thankyou': 1279, 'followed': 493, 'cream': 301, 'finals': 473, 'wants': 1400, 'needed': 889, 'vegas': 1380, 'isnt': 678, 'news': 893, 'date': 324, 'fingers': 476, 'crossed': 304, 'thanx': 1280, 'britneyspears': 176, 'bummed': 188, 'gig': 529, 'agree': 20, 'chicken': 234, 'enjoy': 401, 'door': 365, 'walking': 1395, 'limit': 760, 'annoying': 54, 'price': 1006, 'learned': 741, 'man': 817, 'finish': 477, 'harry': 586, 'exactly': 416, 'posted': 996, 'hit': 618, 'city': 247, 'yrs': 1495, 'boy': 165, 'sadly': 1084, 'okay': 921, 'thinks': 1287, 'pool': 990, 'alancarr': 33, 'present': 1004, 'evil': 415, 'lmao': 773, 'moving': 874, 'death': 333, 'exam': 417, 'blackberry': 141, 'proud': 1017, 'posting': 997, 'probs': 1012, 'account': 4, 'sit': 1153, 'jk': 687, 'bigger': 132, 'tho': 1289, 'fab': 430, 'alyssamilano': 42, 'germany': 524, 'hes': 611, 'earlier': 387, 'cook': 282, 'youu': 1493, 'kids': 708, 'retweet': 1064, 'bag': 107, 'san': 1089, 'place': 973, 'figure': 468, 'wearing': 1411, 'line': 761, 'books': 157, 'smh': 1168, 'expensive': 424, 'english': 400, 'beat': 119, 'promise': 1015, 'talked': 1259, 'ages': 18, 'sum': 1237, 'busy': 194, 'supernatural': 1244, 'sleepy': 1161, 'ben': 125, 'ashleyltmsyf': 74, 'extra': 427, 'disney': 356, 'missing': 854, 'ooh': 927, 'tweeted': 1346, 'mtv': 877, 'straight': 1224, 'hows': 638, 'status': 1211, 'hearing': 597, 'click': 252, 'yr': 1494, 'fake': 439, 'facebook': 433, 'chris': 243, 'calling': 202, 'asking': 78, 'kate': 700, 'wake': 1393, 'row': 1079, 'taylor': 1263, 'coldplay': 262, 'kisses': 716, 'worked': 1452, 'shopping': 1132, 'boyfriend': 166, 'ah': 22, 'gutted': 564, 'eating': 393, 'major': 811, 'ahh': 26, 'ring': 1068, 'class': 248, 'girls': 532, 'style': 1234, 'possible': 994, 'true': 1332, 'interesting': 668, 'ill': 656, 'upgrade': 1370, 'water': 1408, 'planned': 977, 'apps': 66, 'pro': 1007, 'sitting': 1155, 'mark': 819, 'wrote': 1466, 'asked': 77, 'saying': 1095, 'lately': 733, 'umm': 1362, 'explain': 426, 'personal': 960, 'number': 909, 'cup': 306, 'speaking': 1194, 'form': 501, 'imagine': 660, 'driving': 378, 'rofl': 1074, 'months': 865, 'review': 1065, 'sam': 1088, 'em': 395, 'holiday': 622, 'set': 1118, 'itunes': 681, 'uh': 1359, 'hun': 646, 'shop': 1131, 'quiet': 1027, 'wonderful': 1444, 'mention': 835, 'boring': 159, 'app': 61, 'changed': 222, 'young': 1490, 'calls': 203, 'cd': 217, 'study': 1230, 'test': 1274, 'laugh': 736, 'mac': 806, 'tuesday': 1337, 'ai': 28, 'writing': 1464, 'town': 1320, 'cancelled': 207, 'chill': 237, 'aint': 29, 'tweetdeck': 1345, 'james': 684, 'act': 5, 'ball': 108, 'laundry': 738, 'hah': 569, 'backstreetboys': 104, 'report': 1059, 'gift': 528, 'tip': 1304, 'stick': 1216, 'sunshine': 1242, 'chinese': 239, 'killed': 710, 'answer': 56, 'service': 1117, 'nah': 884, 'parents': 944, 'south': 1189, 'ima': 659, 'everyday': 414, 'boat': 152, 'fb': 457, 'cried': 303, 'mmm': 856, 'ideas': 654, 'background': 103, 'reminds': 1055, 'series': 1114, 'checked': 228, 'exciting': 421, 'stories': 1221, 'latest': 735, 'abt': 2, 'idk': 655, 'brilliant': 174, 'tickets': 1298, 'jump': 697, 'schedule': 1099, 'daughter': 325, 'afternoon': 16, 'north': 903, 'west': 1425, 'passed': 949, 'internet': 669, 'stayed': 1213, 'thnx': 1288, 'somebody': 1173, 'al': 32, 'knowing': 722, 'random': 1037, 'jon': 694, 'hilarious': 616, 'hand': 576, 'quit': 1028, 'normal': 902, 'jack': 683, 'bread': 170, 'hands': 577, 'obviously': 912, 'june': 698, 'bar': 111, 'famous': 444, 'takes': 1255, 'wife': 1429, 'loss': 791, 'feelin': 460, 'bitch': 139, 'pls': 984, 'paper': 943, 'public': 1019, 'luv': 805, 'heat': 599, 'wat': 1404, 'imma': 661, 'jus': 699, 'strong': 1228, 'bummer': 189, 'john': 690, 'tom': 1309, 'bsb': 184, 'praying': 1002, 'chicago': 233, 'uk': 1360, 'recommend': 1047, 'pink': 970, 'grow': 559, 'bf': 129, 'count': 290, 'bgt': 130, 'links': 764, 'channel': 223, 'choose': 242, 'outside': 936, 'touch': 1317, 'posts': 998, 'billyraycyrus': 135, 'figured': 469, 'support': 1245, 'cos': 287, 'recently': 1046, 'hungry': 647, 'text': 1275, 'watched': 1406, 'travel': 1326, 'wishing': 1437, 'marsiscoming': 821, 'final': 471, 'difference': 350, 'dark': 320, 'matter': 823, 'album': 34, 'issue': 679, 'complete': 273, 'gorgeous': 549, 'simple': 1145, 'spam': 1191, 'st': 1201, 'starting': 1207, 'taken': 1254, 'cats': 214, 'direct': 354, 'messages': 839, 'played': 981, 'lack': 726, 'doesnt': 359, 'freakin': 504, 'soul': 1185, 'floor': 484, 'games': 520, 'bright': 173, 'record': 1048, 'sushi': 1249, 'server': 1116, 'ahead': 25, 'miley': 844, 'taste': 1262, 'dontyouhate': 364, 'boss': 160, 'excellent': 419, 'business': 193, 'share': 1123, 'double': 366, 'hates': 588, 'ahaha': 24, 'kick': 705, 'biggest': 133, 'chillin': 238, 'total': 1315, 'completely': 274, 'spanish': 1192, 'cookies': 283, 'xoxo': 1473, 'earth': 389, 'choice': 241, 'twit': 1351, 'paying': 952, 'sort': 1184, 'art': 70, 'allowed': 38, 'yawn': 1479, 'woo': 1447, 'dumb': 383, 'closer': 255, 'secret': 1105, 'lakers': 729, 'blood': 149, 'beauty': 121, 'details': 345, 'quote': 1031, 'beach': 117, 'tan': 1261, 'ladies': 727, 'married': 820, 'color': 264, 'decide': 334, 'credit': 302, 'flying': 488, 'search': 1102, 'fat': 450, 'admit': 11, 'hee': 600, 'oops': 929, 'helps': 610, 'deal': 331, 'vote': 1389, 'trust': 1333, 'happens': 583, 'fabulous': 431, 'points': 989, 'studying': 1231, 'bike': 134, 'agreed': 21, 'positive': 993, 'jonas': 695, 'brothers': 181, 'smell': 1167, 'lazy': 739, 'screen': 1101, 'questions': 1024, 'street': 1226, 'fit': 480, 'entire': 404, 'training': 1325, 'design': 344, 'spent': 1197, 'rough': 1077, 'hold': 621, 'single': 1149, 'film': 470, 'vid': 1382, 'sooooo': 1181, 'learning': 742, 'yeh': 1485, 'remind': 1054, 'hrs': 639, 'inside': 665, 'beer': 123, 'sending': 1111, 'finding': 474, 'offer': 914, 'gosh': 550, 'ride': 1066, 'joke': 692, 'canada': 206, 'online': 925, 'turns': 1341, 'interview': 670, 'strange': 1225, 'aha': 23, 'babies': 100, 'worries': 1457, 'putting': 1022, 'spread': 1199, 'cough': 289, 'current': 307, 'yah': 1477, 'perfect': 957, 'thursday': 1295, 'msn': 876, 'holidays': 623, 'vacation': 1379, 'key': 704, 'mobile': 858, 'roll': 1075, 'lesson': 748, 'cali': 200, 'memories': 832, 'problems': 1011, 'clear': 251, 'yup': 1499, 'rainy': 1035, 'shout': 1135, 'bing': 136, 'nooo': 899, 'laughing': 737, 'blah': 142, 'moved': 871, 'pants': 942, 'kitty': 718, 'pc': 953, 'diet': 349, 'atl': 82, 'plane': 976, 'depressing': 342, 'load': 775, 'smart': 1166, 'episodes': 407, 'killing': 711, 'blocked': 147, 'youre': 1491, 'upset': 1372, 'view': 1385, 'cheese': 232, 'fave': 454, 'tha': 1276, 'available': 88, 'bunch': 190, 'mins': 849, 'letting': 751, 'yu': 1496, 'sense': 1112, 'gotten': 552, 'piece': 969, 'grab': 553, 'storm': 1222, 'honey': 627, 'child': 235, 'epic': 405, 'hubby': 641, 'august': 86, 'tweeps': 1343, 'afraid': 15, 'folks': 491, 'return': 1063, 'legs': 747, 'warm': 1401, 'island': 677, 'dr': 369, 'shower': 1136, 'block': 146, 'oo': 926, 'paris': 945, 'throat': 1292, 'anoopdoggdesai': 55, 'goodsex': 547, 'future': 518, 'fault': 452, 'ily': 657, 'sims': 1146, 'shes': 1126, 'taking': 1256, 'tix': 1306, 'american': 48, 'awards': 91, 'kept': 703, 'trailer': 1323, 'classes': 249, 'nose': 904, 'history': 617, 'bear': 118, 'sale': 1087, 'fact': 434, 'hopefully': 629, 'lives': 771, 'heading': 594, 'station': 1210, 'issues': 680, 'hella': 605, 'starts': 1208, 'brown': 183, 'tiny': 1303, 'access': 3, 'space': 1190, 'tooo': 1313, 'midnight': 842, 'drinks': 376, 'situation': 1156, 'wwwtweeteraddercom': 1468, 'oooh': 928, 'anytime': 58, 'notice': 906, 'garden': 521, 'cheap': 226, 'ouch': 935, 'results': 1062, 'feed': 458, 'stage': 1202, 'haveyouever': 591, 'given': 533, 'fam': 442, 'hehehe': 603, 'waste': 1403, 'ash': 72, 'odd': 913, 'barely': 112, 'drop': 379, 'weight': 1421, 'amandaholden': 43, 'stress': 1227, 'invited': 672, 'drama': 370, 'doubt': 367, 'faith': 438, 'opinion': 931, 'kitchen': 717, 'depressed': 341, 'losing': 790, 'leg': 746, 'college': 263, 'advice': 13, 'ny': 910, 'fml': 489, 'helping': 609, 'comp': 271, 'focus': 490, 'weekends': 1419, 'bank': 110, 'throw': 1293, 'added': 9, 'argh': 69, 'heh': 601, 'burn': 191, 'performance': 958, 'voted': 1390, 'tear': 1268, 'outta': 937, 'teach': 1265, 'relaxing': 1050, 'coast': 258, 'mentioned': 836}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=2500, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 1500)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abt</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>actually</th>\n",
       "      <th>adam</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youu</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yu</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  abt  access  account  act  actually  adam  add  added  \\\n",
       "0     0           0    0       0        0    0         0     0    0      0   \n",
       "1     0           0    0       0        0    0         0     0    0      0   \n",
       "2     0           0    0       0        0    0         0     0    0      0   \n",
       "3     0           0    0       0        0    0         0     0    0      0   \n",
       "4     0           0    0       0        0    0         0     0    0      0   \n",
       "\n",
       "   ...  young  youre  youtube  youu  yr  yrs  yu  yum  yummy  yup  \n",
       "0  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "1  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "2  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "3  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "4  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19998, 1500)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abt</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>actually</th>\n",
       "      <th>adam</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youu</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yu</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  abt  access  account  act  actually  adam  add  added  \\\n",
       "0     0           0    0       0        0    0         0     0    0      0   \n",
       "1     0           0    0       0        0    0         0     0    0      0   \n",
       "2     0           0    0       0        0    0         0     0    0      0   \n",
       "3     0           0    0       0        0    0         0     0    0      0   \n",
       "4     0           0    0       0        0    0         0     0    0      0   \n",
       "\n",
       "   ...  young  youre  youtube  youu  yr  yrs  yu  yum  yummy  yup  \n",
       "0  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "1  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "2  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "3  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "4  ...      0      0        0     0   0    0   0    0      0    0  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #vectorize  X_test, uses ame vocabulary as the training dataset so  just call .transform() on X_test\n",
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EDC\\Anaconda3\\envs\\environ1\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'n_estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c0df4d230fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mRFC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'n_estimators'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(df2.SentimentText, min_count=20, window=3, size=300, negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('updates', 0.7657299637794495),\n",
       " ('email', 0.7542745471000671),\n",
       " ('page', 0.752164363861084),\n",
       " ('account', 0.7443256974220276),\n",
       " ('facebook', 0.7436927556991577),\n",
       " ('address', 0.7326928973197937),\n",
       " ('list', 0.7305889129638672),\n",
       " ('myspace', 0.7252993583679199),\n",
       " ('site', 0.7247421145439148),\n",
       " ('comment', 0.719225287437439),\n",
       " ('dm', 0.7131236791610718),\n",
       " ('link', 0.7123754620552063),\n",
       " ('web', 0.7121319770812988),\n",
       " ('message', 0.7109414339065552),\n",
       " ('sent', 0.7066717743873596)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('twitter', topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

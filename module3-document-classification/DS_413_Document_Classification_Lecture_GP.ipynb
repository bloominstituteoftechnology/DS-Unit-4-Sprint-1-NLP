{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtD542Q-3G0F"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ymYc3n3G0G"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 1, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDd8xZ9U3G0H"
      },
      "source": [
        "# Document Classification (Prepare)\n",
        "\n",
        "Today's guided module project will be different. You already know how to do classification. You ready know how to extract features from documents. So? That means you're ready to combine and practice those skills in a kaggle competition. We we will open with a five minute sprint explaining the competition, and then give you 25 minutes to work. After those twenty five minutes are up, I will give a 5-minute demo an NLP technique that will help you with document classification (*and **maybe** the competition*).\n",
        "\n",
        "Today's all about having fun and practicing your skills. The competition will begin\n",
        "\n",
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Text Feature Extraction & Classification Pipelines\n",
        "* <a href=\"#p2\">Part 2</a>: Latent Semantic Indexing\n",
        "* <a href=\"#p3\">Part 3</a>: Word Embeddings with Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghGJqE6e3G0H"
      },
      "source": [
        "# Text Feature Extraction & Classification Pipelines (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmRecfyI3G0I"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Sklearn pipelines allow you to stitch together multiple components of a machine learning process. The idea is that you can pass you raw data and get predictions out of the pipeline. This ability to pass raw input and receive a prediction from a singular class makes pipelines well suited for production, because you can pickle a a pipeline without worry about other data preprocessing steps.\n",
        "\n",
        "*Note:* Each time we call the pipeline during grid search, each component is fit again. The vectorizer (tf-idf) is transforming our entire vocabulary during each cross-validation fold. That transformation adds significant run time to our grid search. There *might* be interactions between the vectorizer and our classifier, so we estimate their performance together in the code below. However, if your goal is to reduce run time. Train your vectorizer separately (ie out of the grid-searched pipeline)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u1FfYBf3G0I"
      },
      "outputs": [],
      "source": [
        "# Import Statements\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "75e17f01bde04e926a348a78398b3d55",
          "grade": false,
          "grade_id": "cell-2d860ec20fad5c0c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bZAR-4273G0J"
      },
      "outputs": [],
      "source": [
        "# Dataset categories (limit to 3 categories vs. 20 to reduce runtime)\n",
        "categories = ['sci.electronics',\n",
        "              'rec.sport.baseball',\n",
        "              'rec.sport.hockey']\n",
        "\n",
        "# Load training data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=categories)\n",
        "\n",
        "# Load testing data\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "\n",
        "print(f'Training Samples: {len(newsgroups_train.data)}')\n",
        "print(f'Testing Samples: {len(newsgroups_test.data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVDGI1j53G0J"
      },
      "outputs": [],
      "source": [
        "type(newsgroups_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWn8zRTt3G0J"
      },
      "outputs": [],
      "source": [
        "dir(newsgroups_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46GRQABN3G0K"
      },
      "outputs": [],
      "source": [
        "newsgroups_train.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Acvwv93f3G0K"
      },
      "outputs": [],
      "source": [
        "newsgroups_train.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk0e7Dyw3G0K"
      },
      "outputs": [],
      "source": [
        "newsgroups_train.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjrWplj3G0L"
      },
      "source": [
        "### Explore the Data\n",
        "\n",
        "Building and training a model is only one part of the workflow. Understanding the basic characteristics of your data beforehand will enable you to build a better model. This could mean obtaining higher accuracy, requiring less data for training, or potentially fewer computation resources.\n",
        "\n",
        "Here we have a few helper functions that will let us explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_words_per_sample(sample_texts):\n",
        "    \"\"\"Gets the median number of words per sample given corpus.\n",
        "\n",
        "    # Arguments\n",
        "        sample_texts: list, sample texts.\n",
        "\n",
        "    # Returns\n",
        "        int, median number of words per sample.\n",
        "    \"\"\"\n",
        "    num_words = [len(s.split()) for s in sample_texts]\n",
        "    return np.median(num_words)\n",
        "\n",
        "def plot_sample_length_distribution(sample_texts):\n",
        "    \"\"\"Plots the sample length distribution.\n",
        "\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "    \"\"\"\n",
        "    plt.hist([len(s.split()) for s in sample_texts], 50)\n",
        "    plt.xlabel('Length of a sample')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Sample length distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
        "                                          num_ngrams=50,\n",
        "                                          **kwargs):\n",
        "    \"\"\"Plots the frequency distribution of n-grams.\n",
        "\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
        "            Min and mplt are the lower and upper bound values for the range.\n",
        "        num_ngrams: int, number of n-grams to plot.\n",
        "            Top `num_ngrams` frequent n-grams will be plotted.\n",
        "    \"\"\"\n",
        "    # Instantiate the vectorizer\n",
        "    vectorizer = CountVectorizer(**kwargs)\n",
        "\n",
        "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
        "    # idxices). This also converts every text to an array the length of\n",
        "    # vocabulary, where every element idxicates the count of the n-gram\n",
        "    # corresponding at that idxex in vocabulary.\n",
        "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
        "\n",
        "    # This is the list of all n-grams in the index order from the vocabulary.\n",
        "    all_ngrams = list(vectorizer.get_feature_names_out())\n",
        "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
        "    # ngrams = all_ngrams[:num_ngrams]\n",
        "\n",
        "    # Add up the counts per n-gram ie. column-wise\n",
        "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
        "\n",
        "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
        "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
        "        zip(all_counts, all_ngrams), reverse=True)])\n",
        "    ngrams = list(all_ngrams)[:num_ngrams]\n",
        "    counts = list(all_counts)[:num_ngrams]\n",
        "\n",
        "    idx = np.arange(num_ngrams)\n",
        "    plt.figure(figsize=(18,6))\n",
        "    plt.bar(idx, counts, width=0.8, color='b')\n",
        "    plt.xlabel('N-grams')\n",
        "    plt.ylabel('Frequencies')\n",
        "    plt.title('Frequency distribution of n-grams')\n",
        "    plt.xticks(idx, ngrams, rotation=45)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FizyHngU4AU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ_z4w2r3G0L"
      },
      "outputs": [],
      "source": [
        "median_words_per_sample = get_num_words_per_sample(newsgroups_train.data)\n",
        "print(f'Median words per sample: {median_words_per_sample}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vjs43MO3G0L"
      },
      "outputs": [],
      "source": [
        "plot_sample_length_distribution(newsgroups_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLGOt5Ww3G0M"
      },
      "outputs": [],
      "source": [
        "# Create args required for vectorizing\n",
        "kwargs = {\n",
        "  'ngram_range': (1, 2),\n",
        "  'stop_words': 'english',\n",
        "  'max_df': 0.25,\n",
        "}\n",
        "\n",
        "plot_frequency_distribution_of_ngrams(newsgroups_train.data,\n",
        "                                      num_ngrams=50,\n",
        "                                      **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7jimiKO3G0M"
      },
      "source": [
        "### Choose a Modeling Approach\n",
        "\n",
        "Reference: https://developers.google.com/machine-learning/guides/text-classification/step-2-5\n",
        "\n",
        "At this point, we have assembled our dataset and gained insights into the key characteristics of our data. Next, based on the metrics we gathered we should think about which classification model we should use. This means asking questions such as:\n",
        "\n",
        "- “How do we present the text data to an algorithm that expects numeric input?” (Data preprocessing and vectorization)\n",
        "- “What type of model should we use?”\n",
        "- “What configuration parameters should we use for our model?”, etc.\n",
        "\n",
        "In the reference guide above, Google attempts to significantly simplify the process of selecting a text classification model. For a given dataset, our goal is to find the algorithm that achieves close to maximum accuracy while minimizing computation time required for training. We ran a large number (~450K) of experiments across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures. This helped us identify dataset parameters that influence optimal choices.\n",
        "\n",
        "The model selection algorithm and flowchart below are a summary of our experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MXUvUl93G0N"
      },
      "outputs": [],
      "source": [
        "sw_ratio = len(newsgroups_train.data) / median_words_per_sample\n",
        "print(f'Number of Samples / Median Words per Sample ratio: {int(sw_ratio)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRWLwOTS3G0N"
      },
      "source": [
        "![](https://developers.google.com/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17ZA4J1L3G0N"
      },
      "source": [
        "## Build a Baseline TF-IDF Model with Support Vector Machine (or Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx5T-uzJ3G0N"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def my_tokenizer(text):\n",
        "    clean_text = re.sub('[^a-zA-Z ]', '', text)\n",
        "    tokens = [token.lower() for token in clean_text.split() if len(token) > 2]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYhbQ8YQ3G0O"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Create Pipeline Components\n",
        "vect = TfidfVectorizer(stop_words='english',\n",
        "                       tokenizer=my_tokenizer)\n",
        "svm = LinearSVC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBzFO1hw3G0O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Get sparse dtm\n",
        "dtm = vect.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Convert to dataframe\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names_out())\n",
        "print(dtm.shape)\n",
        "dtm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySK6qvY73G0O"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "# Define the Pipeline\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2nU9nvZ3G0O"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    'vect__ngram_range': ((1,1), (1,2)),    # Consider unigrams or unigrams/bigrams\n",
        "    'vect__max_df': (0.25, 0.5),            # Removes common words\n",
        "    'vect__min_df': (2, 3, 5),              # Removes rare words\n",
        "    'clf__C': (0.1, 0.5, 1)                 # Strength of regularization penalty (higher -> more)\n",
        "}\n",
        "\n",
        "# Map y from integer label to category\n",
        "y_train = [newsgroups_train.target_names[label] for label in newsgroups_train.target]\n",
        "y_test = [newsgroups_test.target_names[label] for label in newsgroups_test.target]\n",
        "\n",
        "# Run grid search\n",
        "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=4, verbose=1)\n",
        "grid_search.fit(newsgroups_train.data, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdIo9qW63G0O"
      },
      "outputs": [],
      "source": [
        "grid_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWW9hSq83G0P"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JxJ8Dyb3G0P"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate on test data\n",
        "y_test_pred = grid_search.predict(newsgroups_test.data)\n",
        "accuracy_score(y_test, y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6Mpi72F3G0P"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewz22YVH3G0P"
      },
      "source": [
        "## Bonus Section: Explainability"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install eli5"
      ],
      "metadata": {
        "id": "Vq1BzzTx_F0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ7Azsbr3G0P"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "# Extract the vectorizer and model from grid_search pipeline\n",
        "best_model = grid_search.best_estimator_\n",
        "vect = best_model.named_steps['vect']\n",
        "clf = best_model.named_steps['clf']\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_70zwgf3G0P"
      },
      "outputs": [],
      "source": [
        "import eli5\n",
        "\n",
        "eli5.show_weights(clf, vec=vect, top=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "JWWChR-43G0P"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "What you should be doing now:\n",
        "1. Go to the Kaggle competition notebook.\n",
        "3. Train a model & try:\n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "4. Make a submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzE4tXPC3G0Q"
      },
      "source": [
        "## Latent Semantic Indexing (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gO2aKgS3G0Q"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Below is an excellent visual representation of *Latent Semantic Indexing*, also known as LSI, and as *Latent Semantic Analyis*, or LSA.<br>\n",
        "Latent Semantic Indexing is a technique for Topic  Modeling, i.e. grouping a corpus of documents into similar clusters that can then be examined for topics. In the graphic below read \"context\" as \"topics\"<br>\n",
        "\n",
        "Grouping documents into clusters with different topics is also a form of dimensionality reduction, because the document can be represented by a vector of topics instead of a vector of tokens. <br>\n",
        "\n",
        "If your document-term matrix has $m$ documents and $n$ terms, <br>and the number of topics you want to find is $k$, then:\n",
        "- The Term-Document Matrix is $n\\times m$ (the transpose of the Document-Term Matrix)\n",
        "- The Word Assignment to Topics Matrix is $n \\times k$\n",
        "- The Topic Importance Matrix is $k \\times k$\n",
        "- The Topic Distribution Across Documents Matrix is $k \\times m$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "larQHDxN3G0Q"
      },
      "source": [
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20210406165951/Screenshot20210406165933.png)<br>\n",
        "Image Credit: [Geeks for Geeks](https://media.geeksforgeeks.org/)\n",
        "\n",
        "The image above shows a decomposition of the term-document matrix into a product of three matrices.<br>\n",
        "\n",
        "- word = term (token or lemma)\n",
        "- context means \"topic\" or document grouping\n",
        "\n",
        "In the term-document matrix, the columns are vector representations of documents\n",
        "\n",
        "In the words-context matrix, the columns represent relative weighting of words for each topic\n",
        "\n",
        "In the context-document matrix, the columns represent relative weighting of topics for each document\n",
        "\n",
        "**Take Aways:** LSA has two main benefits\n",
        "\n",
        "1. Dimensionality Reduction (because you can choose the number of topics $k \\ll n$)\n",
        "2. Topic Modeling is a form of feature engineering, because it identifies latent (hidden) topics that are present in our doc-term matrix. <br>\n",
        "This is something that counting vectorizers can't do (i.e. `CountVectorizer`, `TfidfVectorizer`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7GF4t9a3G0Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Instantiate Singluar Value Decompsition class\n",
        "svd = TruncatedSVD(n_components=2,\n",
        "                   algorithm='randomized',\n",
        "                   n_iter=10)\n",
        "\n",
        "data = ['pizza', 'pizza hamburger cookie', 'hamburger',\n",
        "        'ramen', 'sushi', 'ramen sushi']\n",
        "\n",
        "vec = CountVectorizer()\n",
        "dtm = vec.fit_transform(data)\n",
        "dtm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4gUi02t3G0R"
      },
      "outputs": [],
      "source": [
        "# Reduce dimensionality of dtm with SVD\n",
        "X_reduced = svd.fit_transform(dtm)\n",
        "X_reduced.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy3I0gS73G0R"
      },
      "outputs": [],
      "source": [
        "# View as dataframe\n",
        "dtm_reduced = pd.DataFrame(X_reduced.round(2), columns=['topic_1', 'topic_2'])\n",
        "dtm_reduced.index = data\n",
        "dtm_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "830086103f96952c9a6ce902226f9803",
          "grade": false,
          "grade_id": "cell-0ff7ed88cbc5eb32",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "yE90Pp3_3G0R"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "# Define a text classification pipeline with LSI\n",
        "\n",
        "vect = TfidfVectorizer(stop_words='english',\n",
        "                       ngram_range=(1,2),\n",
        "                       min_df=3,\n",
        "                       max_df=0.25)\n",
        "svm = LinearSVC(C=0.5, penalty='l2')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7KX2-1a3G0R"
      },
      "outputs": [],
      "source": [
        "# Run a grid search\n",
        "parameters = {\n",
        "    'svd__n_components': (100, 500, 1000),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=4, verbose=1)\n",
        "grid_search.fit(newsgroups_train.data, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6de5scBh3G0R"
      },
      "outputs": [],
      "source": [
        "grid_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD2YCiEy3G0R"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJacI_6c3G0R"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test data\n",
        "y_test_pred = grid_search.predict(newsgroups_test.data)\n",
        "accuracy_score(y_test, y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHpWJ6tU3G0S"
      },
      "outputs": [],
      "source": [
        "# Print the classification report\n",
        "print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "rj7Py24F3G0S"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "What you should be doing now:\n",
        "1. Go to the Kaggle competition notebook.\n",
        "3. Train a model & try:\n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - -> Add Latent Semantic Indexing (lsi) into your pipeline.\n",
        "4. Make a submission to Kaggle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqx7DXLm3G0S"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Continue to apply Latent Semantic Indexing (LSI) to various datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJp3zlPs3G0S"
      },
      "source": [
        "# Word Embeddings with Spacy (Learn)\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k-vXScL3G0S"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "eSpSXFFs3BU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart runtime!"
      ],
      "metadata": {
        "id": "Z6n7TVwg3ucZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "996d16297087ad9c8acbfbeb726b15bb",
          "grade": false,
          "grade_id": "cell-30f6f3d27deb63a3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "4F8cCL2C3G0S"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# load in pre-trained glove model\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Dataset categories (limit to 3 categories vs. 20 to reduce runtime)\n",
        "categories = ['sci.electronics',\n",
        "              'rec.sport.baseball',\n",
        "              'rec.sport.hockey']\n",
        "\n",
        "# Load training data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=categories)\n",
        "\n",
        "# Load testing data\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "\n",
        "print(f'Training Samples: {len(newsgroups_train.data)}')\n",
        "print(f'Testing Samples: {len(newsgroups_test.data)}')"
      ],
      "metadata": {
        "id": "uIbbuiaE3rrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map y from integer label to category\n",
        "y_train = [newsgroups_train.target_names[label] for label in newsgroups_train.target]\n",
        "y_test = [newsgroups_test.target_names[label] for label in newsgroups_test.target]"
      ],
      "metadata": {
        "id": "isFKjdOT4COj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIhxKCB43G0S"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"Natural lanugage processing is awesome!\")\n",
        "doc.vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPocaGd23G0S"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "# Define a function to return average word vector for a given document\n",
        "def get_avg_word_vectors(docs):\n",
        "    return [nlp(doc).vector for doc in docs]\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5wyV-jt3G0T"
      },
      "outputs": [],
      "source": [
        "X = get_avg_word_vectors(newsgroups_train.data)\n",
        "len(X) == len(newsgroups_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU_Xy1RL3G0T"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(C=0.5, penalty='l2')\n",
        "svm.fit(X, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoCRTEoC3G0T"
      },
      "outputs": [],
      "source": [
        "X_test = get_avg_word_vectors(newsgroups_test.data)\n",
        "len(X_test) == len(newsgroups_test.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr9sKkeZ3G0T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_test_pred = svm.predict(X_test)\n",
        "accuracy_score(y_test, y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "51vAibEh14c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnI4h6573G0T"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hpbyRn3G0T"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "What you should be doing now:\n",
        "1. Go to the Kaggle competition notebook.\n",
        "3. Train a model & try:\n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (lsi) into your pipeline.\n",
        "    - -> To extract word embeddings with Spacy and use those embeddings as your features for a classification model.\n",
        "4. Make a submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGJn44CI3G0T"
      },
      "source": [
        "# Review\n",
        "\n",
        "To review this module:\n",
        "* Continue working on the Kaggle competition\n",
        "* Find another text classification task to work on"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S1-NLP (Python3)",
      "language": "python",
      "name": "u4-s1-nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JWWChR-43G0P",
        "dzE4tXPC3G0Q",
        "rj7Py24F3G0S",
        "Nqx7DXLm3G0S",
        "XnI4h6573G0T",
        "f7hpbyRn3G0T"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
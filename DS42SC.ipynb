{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:32.873779Z",
     "start_time": "2019-03-29T17:50:32.868361Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:33.434582Z",
     "start_time": "2019-03-29T17:50:33.426771Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(whitespace_string.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:34.336326Z",
     "start_time": "2019-03-29T17:50:34.332307Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:35.006635Z",
     "start_time": "2019-03-29T17:50:34.747230Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt\"\n",
    "text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:35.209546Z",
     "start_time": "2019-03-29T17:50:35.174936Z"
    }
   },
   "outputs": [],
   "source": [
    "ml = re.findall(\"(\\w+)\\s*(\\d{1,2})\\,\\s*(\\d+)\", text)\n",
    "df = pd.DataFrame(ml, columns = ['Month','Day','Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:36.169063Z",
     "start_time": "2019-03-29T17:50:36.165101Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:39.124431Z",
     "start_time": "2019-03-29T17:50:36.698578Z"
    }
   },
   "outputs": [],
   "source": [
    "text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:42.867912Z",
     "start_time": "2019-03-29T17:50:39.129493Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:45.513536Z",
     "start_time": "2019-03-29T17:50:42.869797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>cleanedSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, already]</td>\n",
       "      <td>omg already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "      <td>omgaga im sooo im gunna cry ive dentist since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "      <td>think mi bf cheating tt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0                                 [sad, apl, friend]   \n",
       "1                       [missed, new, moon, trailer]   \n",
       "2                                     [omg, already]   \n",
       "3  [omgaga, im, sooo, im, gunna, cry, ive, dentis...   \n",
       "4                      [think, mi, bf, cheating, tt]   \n",
       "\n",
       "                                     cleanedSentence  \n",
       "0                                     sad apl friend  \n",
       "1                            missed new moon trailer  \n",
       "2                                        omg already  \n",
       "3  omgaga im sooo im gunna cry ive dentist since ...  \n",
       "4                            think mi bf cheating tt  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "# \tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "def clean_sentence(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "# \tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "\treturn \" \".join(tokens)\n",
    "\n",
    "df['cleaned'] = df.SentimentText.apply(clean_doc)\n",
    "df['cleanedSentence'] = df.SentimentText.apply(clean_sentence)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:46.634914Z",
     "start_time": "2019-03-29T17:50:45.516386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,1), stop_words='english')\n",
    "vectorizer.fit(df.cleanedSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TF-IDF the higher rated words have more discriminative power than the lower rated words because they more specifically identify the category.    The TF-IDF for a term is the product of its term frequency and the scaled inverse of its document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:54.165799Z",
     "start_time": "2019-03-29T17:50:54.162824Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df.cleanedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:57.723419Z",
     "start_time": "2019-03-29T17:50:55.005352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aaahh</th>\n",
       "      <th>aafreen</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahhh</th>\n",
       "      <th>aalaap</th>\n",
       "      <th>aamyhaanson</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aaah  aaahh  aafreen  aah  aahhh  aalaap  aamyhaanson  aaron  \\\n",
       "0  0.0  0.0   0.0    0.0      0.0  0.0    0.0     0.0          0.0    0.0   \n",
       "1  0.0  0.0   0.0    0.0      0.0  0.0    0.0     0.0          0.0    0.0   \n",
       "2  0.0  0.0   0.0    0.0      0.0  0.0    0.0     0.0          0.0    0.0   \n",
       "3  0.0  0.0   0.0    0.0      0.0  0.0    0.0     0.0          0.0    0.0   \n",
       "4  0.0  0.0   0.0    0.0      0.0  0.0    0.0     0.0          0.0    0.0   \n",
       "\n",
       "   ...  zero  zip  zombie  zombies  zomg  zone  zones  zoo  zoom  zune  \n",
       "0  ...   0.0  0.0     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  \n",
       "1  ...   0.0  0.0     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  \n",
       "2  ...   0.0  0.0     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  \n",
       "3  ...   0.0  0.0     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  \n",
       "4  ...   0.0  0.0     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts[0:10000].toarray(), columns=vectorizer.get_feature_names())\n",
    "X_train_vectorized = X_train_vectorized.fillna(0)\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:51:09.809785Z",
     "start_time": "2019-03-29T17:51:09.761256Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "dfs = df.sample(frac=0.1)\n",
    "X = dfs.cleanedSentence\n",
    "y = dfs.Sentiment.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:51:11.508193Z",
     "start_time": "2019-03-29T17:51:11.504530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7999,)\n",
      "(2000,)\n",
      "(7999,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:51:16.482859Z",
     "start_time": "2019-03-29T17:51:16.361378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000, ngram_range=(1,1), stop_words='english')\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:51:18.167707Z",
     "start_time": "2019-03-29T17:51:18.059599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7999, 1000)\n",
      "(2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "test_word_counts = vectorizer.transform(X_test)\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(X_train_vectorized.shape)\n",
    "print(X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:51:21.222904Z",
     "start_time": "2019-03-29T17:51:21.218619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:48:51.962715Z",
     "start_time": "2019-03-29T17:48:31.500744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7029628703587949\n",
      "Test Accuracy: 0.668\n",
      "Train Roc Auc: 0.674452463770322\n",
      "Test Roc Auc: 0.6434984820102061\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBClassifier(n_estimators=200, objective=\"binary:logistic\").fit(X_train_vectorized, y_train)\n",
    "train_predictions = XGB.predict(X_train_vectorized)\n",
    "test_predictions = XGB.predict(X_test_vectorized)\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "print(f'Train Roc Auc: {roc_auc_score(y_train, train_predictions)}')\n",
    "print(f'Test Roc Auc: {roc_auc_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:49:33.866072Z",
     "start_time": "2019-03-29T17:48:53.730994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9523690461307663\n",
      "Test Accuracy: 0.6885\n",
      "Train Roc Auc: 0.9482529265359557\n",
      "Test Roc Auc: 0.6829379562043796\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=200).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "print(f'Train Roc Auc: {roc_auc_score(y_train, train_predictions)}')\n",
    "print(f'Test Roc Auc: {roc_auc_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:49:38.271311Z",
     "start_time": "2019-03-29T17:49:36.806563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7688461057632204\n",
      "Test Accuracy: 0.6925\n",
      "Train Roc Auc: 0.7568833533763624\n",
      "Test Roc Auc: 0.6816460499967703\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=42, solver=\"newton-cg\").fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "print(f'Train Roc Auc: {roc_auc_score(y_train, train_predictions)}')\n",
    "print(f'Test Roc Auc: {roc_auc_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:49:44.477462Z",
     "start_time": "2019-03-29T17:49:41.187186Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:49:50.228485Z",
     "start_time": "2019-03-29T17:49:48.850701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                             cleaned  \n",
       "0                                 [sad, apl, friend]  \n",
       "1                       [missed, new, moon, trailer]  \n",
       "2                                     [omg, already]  \n",
       "3  [omgaga, im, sooo, im, gunna, cry, ive, dentis...  \n",
       "4                      [think, mi, bf, cheating, tt]  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'] = df.SentimentText.apply(clean_doc)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:49:54.233847Z",
     "start_time": "2019-03-29T17:49:51.677163Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec(df.cleaned) # no parameters made the best result in my opinion, only with that include \"twitt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:49:55.205024Z",
     "start_time": "2019-03-29T17:49:55.197482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('list', 0.8101412057876587),\n",
       " ('facebook', 0.8083671927452087),\n",
       " ('following', 0.7973533868789673),\n",
       " ('brittanyasnow', 0.777572751045227),\n",
       " ('link', 0.7775703072547913),\n",
       " ('dm', 0.7766029238700867),\n",
       " ('awalliewall', 0.766107439994812),\n",
       " ('page', 0.761616051197052),\n",
       " ('grats', 0.7601321935653687),\n",
       " ('aboard', 0.7586065530776978)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('twitter', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:01.249867Z",
     "start_time": "2019-03-29T17:50:01.246421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.doesnt_match(['twitter', 'facebook', 'moon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:50:03.455201Z",
     "start_time": "2019-03-29T17:50:03.450876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('email', 0.7282893657684326),\n",
       " ('dm', 0.7136653065681458),\n",
       " ('list', 0.6973550319671631),\n",
       " ('following', 0.6965770721435547),\n",
       " ('sent', 0.6892113089561462),\n",
       " ('celiaistall', 0.6811996698379517),\n",
       " ('followers', 0.6794509291648865),\n",
       " ('follow', 0.6751104593276978),\n",
       " ('link', 0.6682579517364502),\n",
       " ('message', 0.6654179096221924)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"twitter\", \"facebook\"], negative=[\"moon\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

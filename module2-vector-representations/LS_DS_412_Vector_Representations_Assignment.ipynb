{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Vector Representations\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2* Jonatan Rivera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import Statements \"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "hyj-f9FDcVFp",
    "outputId": "5dd045fe-6e4c-458c-e2fc-253c3da9c805"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7bcmqfGXrFG"
   },
   "source": [
    "## 1) *Clean:* Job Listings from indeed.com that contain the title \"Data Scientist\" \n",
    "\n",
    "You have `job_listings.csv` in the data folder for this module. The text data in the description column is still messy - full of html tags. Use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to clean up this column. You will need to read through the documentation to accomplish this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcYlc1URXhlC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;\\nConceptual understanding in Machine Learning mod...</td>\n",
       "      <td>Data scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Data Scientist 1, you will help us build machine lea...</td>\n",
       "      <td>Data Scientist I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0  b\"<div><div>Job Requirements:</div><ul><li><p>\\nConceptual understanding in Machine Learning mod...   \n",
       "1  b'<div>Job Description<br/>\\n<br/>\\n<p>As a Data Scientist 1, you will help us build machine lea...   \n",
       "\n",
       "              title  \n",
       "0   Data scientist   \n",
       "1  Data Scientist I  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#display all text \n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "#convert joblisting csv into data frame \n",
    "df = pd.read_csv(\"data/job_listings.csv\")\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to extract descritption from html \n",
    "def text_extractor(x):\n",
    "    \"\"\"convert html to text \"\"\"\n",
    "    soup = BeautifulSoup(x, 'html.parser')\n",
    "    x = soup.get_text()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text extractor function\n",
    "df['description'] = df['description'].apply(text_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426 entries, 0 to 425\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unnamed: 0   426 non-null    int64 \n",
      " 1   description  426 non-null    object\n",
      " 2   title        426 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 10.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0  \\\n",
       " 0  0            \n",
       " 1  1            \n",
       " 2  2            \n",
       " 3  3            \n",
       " 4  4            \n",
       " \n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    description  \\\n",
       " 0  b\"Job Requirements:\\nConceptual understanding in Machine Learning models like Nai\\xc2\\xa8ve Bayes, K-Means, SVM, Apriori, Linear/ Logistic Regression, Neural, Random Forests, Decision Trees, K-NN along with hands-on experience in at least 2 of them\\nIntermediate to expert level coding skills in Python/R. (Ability to write functions, clean and efficient data manipulation are mandatory for this role)\\nExposure to packages like NumPy, SciPy, Pandas, Matplotlib etc in Python or GGPlot2, dplyr, tidyR in R\\nAbility to communicate Model findings to both Technical and Non-Technical stake holders\\nHands on experience in SQL/Hive or similar programming language\\nMust show past work via GitHub, Kaggle or any other published article\\nMaster's degree in Statistics/Mathematics/Computer Science or any other quant specific field.\\nApply Now\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       " 1  b'Job Description\\n\\nAs a Data Scientist 1, you will help us build machine learning models, data pipelines, and micro-services to help our clients navigate their healthcare journey. You will do so by empowering and improving the next generation of Accolade Applications and user experiences.\\nA day in the life\\xe2\\x80\\xa6\\nWork with a small agile team to design and develop mobile applications in an iterative fashion.\\nWork with a tight-knit group of development team members in Seattle.\\nContribute to best practices and help guide the future of our applications.\\nOperates effectively as a collaborative member of the development team.\\nOperates effectively as an individual for quick turnaround of enhancements and fixes.\\nResponsible for meeting expectations and deliverables on time with high quality.\\nDrive and implement new features within our mobile applications.\\nPerform thorough manual testing and writing test cases that cover all areas.\\nIdentify new development tools/approaches that will increase code quality, efficiency, and best practices.\\nDevelop and champion the the development processes, coding style guidelines, and architectural designs necessary to innovate and maintain great product quality.\\nEffectively turns design documents and graphics into performant, usable UI.\\nDemonstrates creative, technical, and analytical skills.\\nDemonstrates ability to communicate effectively in both technical and business environments\\n\\nQualifications\\n\\nWhat we are looking for\\xe2\\x80\\xa6\\nMaster\\xe2\\x80\\x99s Degree in Computer Science, Math, or related field.\\nComputer Science fundamentals, as illustrated through algorithm design, problem solving, and complexity analysis.\\nMust have 1+ year real-world experience developing and deploying micro-services or data pipelines\\nMust have a fundamental understanding of key machine learning concepts, such as accuracy measures, cross-validation, and open source machine learning libraries\\nFluent in Python and SQL\\nProficient with writing unit/functional tests and familiar with automation frameworks\\nExperience with cloud infrastructure, such as AWS or Azure, is a plus.\\nExperience with distributed data pipelines, such as a Spark, is a plus.\\nStrong written and oral communication skills.\\nDesire and willingness to work in an Agile, collaborative, innovative, flexible, and team-oriented environment\\nHands-on, detail-oriented, methodical & inquisitive\\nA motivated self-starter with a solid level of experience that quickly grasps complex challenges\\nA skillful communicator with experience working with technical management teams\\n A service oriented person who thinks \"Customer First\"\\nFast fail entrepreneurial spirit\\nThrives in a fast-paced environment where continuous improvement is the norm and the bar for quality is extremely high\\nExcited by the challenges of working in a product team undergoing rapid, international growth\\nAdditional Information\\n\\nWhat is important to us\\nCreating an enduring company that is hyper-focused on our culture and making a meaningful impact in the lives of our employees, members and customers. The secret to our success is:\\nWe find joy and purpose in serving others\\nMaking a difference in our members\\xe2\\x80\\x99 and customers\\xe2\\x80\\x99 lives is what we do. Even when it\\xe2\\x80\\x99s hard, we do the right thing for the right reasons.\\nWe are strong individually and together, we\\xe2\\x80\\x99re powerful\\nTrusting in our colleagues and embracing their different backgrounds and experiences enable us to solve tough problems in creative ways, having fun along the way.\\nWe roll up our sleeves and get stuff done\\nResults motivate us. And we aren\\'t afraid of the hard work or tough decisions needed to get us there.\\nWe\\xe2\\x80\\x99re boldly and relentlessly reinventing healthcare\\nWe\\'re curious and act big - not afraid to knock down barriers or take calculated risks to change the world, one person at a time.\\nAll your information will be kept confidential according to EEO guidelines.'   \n",
       " 2  b'As a Data Scientist you will be working on consulting side of our business. You will be responsible for analyzing large, complex datasets and identify meaningful patterns that lead to actionable recommendations. You will be performing thorough testing and validation of models, and support various aspects of the business with data analytics.\\nAbility to do statistical modeling, build predictive models and leverage machine learning algorithms.\\nThis position will combine the typical Data Scientist math and analytical skills, with research, advanced business, communication, and presentation skills.\\nPrimary job location is in Sacramento, but work-from-home option is available.\\n\\nQualifications\\nBachelors, MS or PhD in a relevant field (Computer Science, Engineering, Statistics, Physics, Applied Math)\\nExperience in R and/or Python is preferred'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       " 3  b'$4,969 - $6,756 a monthContractUnder the general supervision of Professors Dana Mukamel and Kai Zheng, the incumbent will join the CalMHSA Mental Health Tech Suite Innovation (INN) Evaluation Team. This large, statewide multi-year study will evaluate the effectiveness of two new and innovative applications offered to people with mental health conditions, which include opportunities for online chatting between users and online listeners Responsibilities of the incumbent will include managing and analyzing text data created by users of the two mental health applications as part of the research and evaluation objectives of the team. The incumbent will collaborate with faculty and other team researchers, and will be expected to create under supervision and direction variables describing the usage of the apps, the interactions between users, and the effectiveness of the apps. The incumbent will also be expected to interact with the vendors of the apps around data issues.\\n\\nThe University of California, Irvine is an Equal Opportunity/Affirmative Action Employer advancing inclusive excellence. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, protected veteran status, or other protected categories covered by the UC nondiscrimination policy.\\n\\nSalary: Monthly $4,968.58 - $6,755.83\\nTotal Hours: 8-5, M-F\\nContract Position.\\nFinal candidate subject to background check.\\nAs a federal contractor, UC Irvine is required to use E-Verify to confirm the work status of individuals assigned to perform substantial work under certain federal contracts/subcontracts.\\n\\nPlease attach your resume.'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " 4  b'Location: USA \\xe2\\x80\\x93 multiple locations\\n2+ years of Analytics experience\\nUnderstand business requirements and technical requirements\\nCan handle data extraction, preparation and transformation\\nCreate and implement data models'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       " \n",
       "                           title  \n",
       " 0  Data scientist                \n",
       " 1  Data Scientist I              \n",
       " 2  Data Scientist - Entry Level  \n",
       " 3  Data Scientist                \n",
       " 4  Data Scientist                ,\n",
       " None)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() , df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further clean data frame\n",
    "df.replace({'\\\\xe2\\\\x80\\\\x99':\"\",\"\\\\xe2\\\\x80\\\\x93\":\"\",\"\\\\xe2\\\\x80\\\\x99\":\"\",\"xe2x80x93\":\"\"},inplace=True)\n",
    "df.replace({r'\\\\n':'', r\"[^a-zA-Z 0-9]\":\"\"}, regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bJob RequirementsConceptual understanding in Machine Learning models like Naixc2xa8ve Bayes KMeans SVM Apriori Linear Logistic Regression Neural Random Forests Decision Trees KNN along with handson experience in at least 2 of themIntermediate to expert level coding skills in PythonR Ability to write functions clean and efficient data manipulation are mandatory for this roleExposure to packages like NumPy SciPy Pandas Matplotlib etc in Python or GGPlot2 dplyr tidyR in RAbility to communicate Model findings to both Technical and NonTechnical stake holdersHands on experience in SQLHive or similar programming languageMust show past work via GitHub Kaggle or any other published articleMasters degree in StatisticsMathematicsComputer Science or any other quant specific fieldApply Now</td>\n",
       "      <td>Data scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bJob DescriptionAs a Data Scientist 1 you will help us build machine learning models data pipelines and microservices to help our clients navigate their healthcare journey You will do so by empowering and improving the next generation of Accolade Applications and user experiencesA day in the lifexe2x80xa6Work with a small agile team to design and develop mobile applications in an iterative fashionWork with a tightknit group of development team members in SeattleContribute to best practices and help guide the future of our applicationsOperates effectively as a collaborative member of the development teamOperates effectively as an individual for quick turnaround of enhancements and fixesResponsible for meeting expectations and deliverables on time with high qualityDrive and implement new features within our mobile applicationsPerform thorough manual testing and writing test cases that cover all areasIdentify new development toolsapproaches that will increase code quality efficiency and best practicesDevelop and champion the the development processes coding style guidelines and architectural designs necessary to innovate and maintain great product qualityEffectively turns design documents and graphics into performant usable UIDemonstrates creative technical and analytical skillsDemonstrates ability to communicate effectively in both technical and business environmentsQualificationsWhat we are looking forxe2x80xa6Masterxe2x80x99s Degree in Computer Science Math or related fieldComputer Science fundamentals as illustrated through algorithm design problem solving and complexity analysisMust have 1 year realworld experience developing and deploying microservices or data pipelinesMust have a fundamental understanding of key machine learning concepts such as accuracy measures crossvalidation and open source machine learning librariesFluent in Python and SQLProficient with writing unitfunctional tests and familiar with automation frameworksExperience with cloud infrastructure such as AWS or Azure is a plusExperience with distributed data pipelines such as a Spark is a plusStrong written and oral communication skillsDesire and willingness to work in an Agile collaborative innovative flexible and teamoriented environmentHandson detailoriented methodical  inquisitiveA motivated selfstarter with a solid level of experience that quickly grasps complex challengesA skillful communicator with experience working with technical management teams A service oriented person who thinks Customer FirstFast fail entrepreneurial spiritThrives in a fastpaced environment where continuous improvement is the norm and the bar for quality is extremely highExcited by the challenges of working in a product team undergoing rapid international growthAdditional InformationWhat is important to usCreating an enduring company that is hyperfocused on our culture and making a meaningful impact in the lives of our employees members and customers The secret to our success isWe find joy and purpose in serving othersMaking a difference in our membersxe2x80x99 and customersxe2x80x99 lives is what we do Even when itxe2x80x99s hard we do the right thing for the right reasonsWe are strong individually and together wexe2x80x99re powerfulTrusting in our colleagues and embracing their different backgrounds and experiences enable us to solve tough problems in creative ways having fun along the wayWe roll up our sleeves and get stuff doneResults motivate us And we arent afraid of the hard work or tough decisions needed to get us thereWexe2x80x99re boldly and relentlessly reinventing healthcareWere curious and act big  not afraid to knock down barriers or take calculated risks to change the world one person at a timeAll your information will be kept confidential according to EEO guidelines</td>\n",
       "      <td>Data Scientist I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>bAs a Data Scientist you will be working on consulting side of our business You will be responsible for analyzing large complex datasets and identify meaningful patterns that lead to actionable recommendations You will be performing thorough testing and validation of models and support various aspects of the business with data analyticsAbility to do statistical modeling build predictive models and leverage machine learning algorithmsThis position will combine the typical Data Scientist math and analytical skills with research advanced business communication and presentation skillsPrimary job location is in Sacramento but workfromhome option is availableQualificationsBachelors MS or PhD in a relevant field Computer Science Engineering Statistics Physics Applied MathExperience in R andor Python is preferred</td>\n",
       "      <td>Data Scientist  Entry Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>b4969  6756 a monthContractUnder the general supervision of Professors Dana Mukamel and Kai Zheng the incumbent will join the CalMHSA Mental Health Tech Suite Innovation INN Evaluation Team This large statewide multiyear study will evaluate the effectiveness of two new and innovative applications offered to people with mental health conditions which include opportunities for online chatting between users and online listeners Responsibilities of the incumbent will include managing and analyzing text data created by users of the two mental health applications as part of the research and evaluation objectives of the team The incumbent will collaborate with faculty and other team researchers and will be expected to create under supervision and direction variables describing the usage of the apps the interactions between users and the effectiveness of the apps The incumbent will also be expected to interact with the vendors of the apps around data issuesThe University of California Irvine is an Equal OpportunityAffirmative Action Employer advancing inclusive excellence All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability age protected veteran status or other protected categories covered by the UC nondiscrimination policySalary Monthly 496858  675583Total Hours 85 MFContract PositionFinal candidate subject to background checkAs a federal contractor UC Irvine is required to use EVerify to confirm the work status of individuals assigned to perform substantial work under certain federal contractssubcontractsPlease attach your resume</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bLocation USA xe2x80x93 multiple locations2 years of Analytics experienceUnderstand business requirements and technical requirementsCan handle data extraction preparation and transformationCreate and implement data models</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0  0            \n",
       "1  1            \n",
       "2  2            \n",
       "3  3            \n",
       "4  4            \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 description  \\\n",
       "0  bJob RequirementsConceptual understanding in Machine Learning models like Naixc2xa8ve Bayes KMeans SVM Apriori Linear Logistic Regression Neural Random Forests Decision Trees KNN along with handson experience in at least 2 of themIntermediate to expert level coding skills in PythonR Ability to write functions clean and efficient data manipulation are mandatory for this roleExposure to packages like NumPy SciPy Pandas Matplotlib etc in Python or GGPlot2 dplyr tidyR in RAbility to communicate Model findings to both Technical and NonTechnical stake holdersHands on experience in SQLHive or similar programming languageMust show past work via GitHub Kaggle or any other published articleMasters degree in StatisticsMathematicsComputer Science or any other quant specific fieldApply Now                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "1  bJob DescriptionAs a Data Scientist 1 you will help us build machine learning models data pipelines and microservices to help our clients navigate their healthcare journey You will do so by empowering and improving the next generation of Accolade Applications and user experiencesA day in the lifexe2x80xa6Work with a small agile team to design and develop mobile applications in an iterative fashionWork with a tightknit group of development team members in SeattleContribute to best practices and help guide the future of our applicationsOperates effectively as a collaborative member of the development teamOperates effectively as an individual for quick turnaround of enhancements and fixesResponsible for meeting expectations and deliverables on time with high qualityDrive and implement new features within our mobile applicationsPerform thorough manual testing and writing test cases that cover all areasIdentify new development toolsapproaches that will increase code quality efficiency and best practicesDevelop and champion the the development processes coding style guidelines and architectural designs necessary to innovate and maintain great product qualityEffectively turns design documents and graphics into performant usable UIDemonstrates creative technical and analytical skillsDemonstrates ability to communicate effectively in both technical and business environmentsQualificationsWhat we are looking forxe2x80xa6Masterxe2x80x99s Degree in Computer Science Math or related fieldComputer Science fundamentals as illustrated through algorithm design problem solving and complexity analysisMust have 1 year realworld experience developing and deploying microservices or data pipelinesMust have a fundamental understanding of key machine learning concepts such as accuracy measures crossvalidation and open source machine learning librariesFluent in Python and SQLProficient with writing unitfunctional tests and familiar with automation frameworksExperience with cloud infrastructure such as AWS or Azure is a plusExperience with distributed data pipelines such as a Spark is a plusStrong written and oral communication skillsDesire and willingness to work in an Agile collaborative innovative flexible and teamoriented environmentHandson detailoriented methodical  inquisitiveA motivated selfstarter with a solid level of experience that quickly grasps complex challengesA skillful communicator with experience working with technical management teams A service oriented person who thinks Customer FirstFast fail entrepreneurial spiritThrives in a fastpaced environment where continuous improvement is the norm and the bar for quality is extremely highExcited by the challenges of working in a product team undergoing rapid international growthAdditional InformationWhat is important to usCreating an enduring company that is hyperfocused on our culture and making a meaningful impact in the lives of our employees members and customers The secret to our success isWe find joy and purpose in serving othersMaking a difference in our membersxe2x80x99 and customersxe2x80x99 lives is what we do Even when itxe2x80x99s hard we do the right thing for the right reasonsWe are strong individually and together wexe2x80x99re powerfulTrusting in our colleagues and embracing their different backgrounds and experiences enable us to solve tough problems in creative ways having fun along the wayWe roll up our sleeves and get stuff doneResults motivate us And we arent afraid of the hard work or tough decisions needed to get us thereWexe2x80x99re boldly and relentlessly reinventing healthcareWere curious and act big  not afraid to knock down barriers or take calculated risks to change the world one person at a timeAll your information will be kept confidential according to EEO guidelines   \n",
       "2  bAs a Data Scientist you will be working on consulting side of our business You will be responsible for analyzing large complex datasets and identify meaningful patterns that lead to actionable recommendations You will be performing thorough testing and validation of models and support various aspects of the business with data analyticsAbility to do statistical modeling build predictive models and leverage machine learning algorithmsThis position will combine the typical Data Scientist math and analytical skills with research advanced business communication and presentation skillsPrimary job location is in Sacramento but workfromhome option is availableQualificationsBachelors MS or PhD in a relevant field Computer Science Engineering Statistics Physics Applied MathExperience in R andor Python is preferred                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "3  b4969  6756 a monthContractUnder the general supervision of Professors Dana Mukamel and Kai Zheng the incumbent will join the CalMHSA Mental Health Tech Suite Innovation INN Evaluation Team This large statewide multiyear study will evaluate the effectiveness of two new and innovative applications offered to people with mental health conditions which include opportunities for online chatting between users and online listeners Responsibilities of the incumbent will include managing and analyzing text data created by users of the two mental health applications as part of the research and evaluation objectives of the team The incumbent will collaborate with faculty and other team researchers and will be expected to create under supervision and direction variables describing the usage of the apps the interactions between users and the effectiveness of the apps The incumbent will also be expected to interact with the vendors of the apps around data issuesThe University of California Irvine is an Equal OpportunityAffirmative Action Employer advancing inclusive excellence All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability age protected veteran status or other protected categories covered by the UC nondiscrimination policySalary Monthly 496858  675583Total Hours 85 MFContract PositionFinal candidate subject to background checkAs a federal contractor UC Irvine is required to use EVerify to confirm the work status of individuals assigned to perform substantial work under certain federal contractssubcontractsPlease attach your resume                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "4  bLocation USA xe2x80x93 multiple locations2 years of Analytics experienceUnderstand business requirements and technical requirementsCan handle data extraction preparation and transformationCreate and implement data models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "                         title  \n",
       "0  Data scientist               \n",
       "1  Data Scientist I             \n",
       "2  Data Scientist  Entry Level  \n",
       "3  Data Scientist               \n",
       "4  Data Scientist               "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4xFZNtX1m2"
   },
   "source": [
    "## 2) Use Spacy to tokenize the listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhUHuMr-X-II"
   },
   "outputs": [],
   "source": [
    "#load nlp model in spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "#create function to eventually use to tokenize description column\n",
    "def get_lemmas(text):\n",
    "    \"\"\" gives you the lemmas for a given text\"\"\"\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON') and (token.is_space == False):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "tokenized_listings = df['description'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [bJob, RequirementsConceptual, understanding, Machine, Learning, model, like, naixc2xa8ve, Bayes, KMeans, SVM, Apriori, Linear, Logistic, regression, Neural, Random, Forests, Decision, Trees, KNN, handson, experience, 2, themintermediate, expert, level, coding, skill, PythonR, ability, write, function, clean, efficient, datum, manipulation, mandatory, roleexposure, package, like, NumPy, SciPy, Pandas, Matplotlib, etc, Python, GGPlot2, dplyr, tidyR, RAbility, communicate, Model, finding, Technical, NonTechnical, stake, holdershand, experience, sqlhive, similar, programming, languageMust, past, work, GitHub, Kaggle, publish, articlemaster, degree, statisticsmathematicscomputer, science, quant, specific, fieldapply]                                                                                                                                                                                                                                                                                         \n",
       "1    [bJob, DescriptionAs, Data, scientist, 1, help, build, machine, learning, model, data, pipeline, microservice, help, client, navigate, healthcare, journey, empower, improve, generation, Accolade, Applications, user, experiencesA, day, lifexe2x80xa6Work, small, agile, team, design, develop, mobile, application, iterative, fashionwork, tightknit, group, development, team, member, SeattleContribute, good, practice, help, guide, future, applicationsoperate, effectively, collaborative, member, development, teamoperate, effectively, individual, quick, turnaround, enhancement, fixesresponsible, meet, expectation, deliverable, time, high, qualityDrive, implement, new, feature, mobile, applicationsperform, thorough, manual, testing, writing, test, case, cover, areasidentify, new, development, toolsapproaches, increase, code, quality, efficiency, good, practicesdevelop, champion, development, process, code, style, guideline, architectural, design, necessary, innovate, maintain, great, product, ...]\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see fist two listings\n",
    "tokenized_listings[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lgCZNL_YycP"
   },
   "source": [
    "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2PZ8Pj_YxcF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnrivera/.virtualenvs/DS-Unit-4-Sprint-1-NLP-N2EbWurJ/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#import count vectorizer, will be need to get word counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#list of text documents\n",
    "job_list = df['description']\n",
    "\n",
    "# create the transformer\n",
    "# use the apply get_lemmas as my tokenizer\n",
    "vect = CountVectorizer(stop_words = 'english', max_df =.90,min_df =2, tokenizer=get_lemmas)\n",
    "\n",
    "# build vocab\n",
    "vect.fit(job_list)\n",
    "\n",
    "# transform text\n",
    "dtm = vect.transform(job_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t1\n",
      "  (0, 107)\t1\n",
      "  (0, 609)\t1\n",
      "  (0, 702)\t1\n",
      "  (0, 1008)\t1\n",
      "  (0, 1054)\t1\n",
      "  (0, 1104)\t1\n",
      "  (0, 1533)\t1\n",
      "  (0, 1564)\t1\n",
      "  (0, 1799)\t1\n",
      "  (0, 1883)\t1\n",
      "  (0, 2135)\t2\n",
      "  (0, 2186)\t1\n",
      "  (0, 2330)\t1\n",
      "  (0, 2393)\t1\n",
      "  (0, 2451)\t1\n",
      "  (0, 2524)\t1\n",
      "  (0, 2527)\t1\n",
      "  (0, 2611)\t1\n",
      "  (0, 3099)\t1\n",
      "  (0, 3116)\t1\n",
      "  (0, 3121)\t1\n",
      "  (0, 3182)\t1\n",
      "  (0, 3208)\t1\n",
      "  (0, 3238)\t2\n",
      "  :\t:\n",
      "  (425, 5656)\t1\n",
      "  (425, 5690)\t3\n",
      "  (425, 5714)\t2\n",
      "  (425, 5786)\t1\n",
      "  (425, 5794)\t1\n",
      "  (425, 5830)\t1\n",
      "  (425, 5885)\t2\n",
      "  (425, 5909)\t1\n",
      "  (425, 5968)\t1\n",
      "  (425, 5980)\t1\n",
      "  (425, 5996)\t1\n",
      "  (425, 6003)\t2\n",
      "  (425, 6048)\t1\n",
      "  (425, 6055)\t1\n",
      "  (425, 6083)\t1\n",
      "  (425, 6095)\t1\n",
      "  (425, 6116)\t1\n",
      "  (425, 6133)\t1\n",
      "  (425, 6156)\t1\n",
      "  (425, 6214)\t1\n",
      "  (425, 6228)\t7\n",
      "  (425, 6233)\t1\n",
      "  (425, 6240)\t1\n",
      "  (425, 6273)\t2\n",
      "  (425, 6295)\t2\n"
     ]
    }
   ],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'todense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-448dfc58b6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get Word Counts for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/DS-Unit-4-Sprint-1-NLP-N2EbWurJ/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'todense'"
     ]
    }
   ],
   "source": [
    "# Get Word Counts for each document\n",
    "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>02</th>\n",
       "      <th>06366</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "      <th>100000125000the</th>\n",
       "      <th>1001schedulefulltimejob</th>\n",
       "      <th>...</th>\n",
       "      <th>youwe</th>\n",
       "      <th>youwhat</th>\n",
       "      <th>youxe2x80x99ll</th>\n",
       "      <th>youxe2x80x99re</th>\n",
       "      <th>youxe2x80x99ve</th>\n",
       "      <th>youyou</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zero</th>\n",
       "      <th>zf</th>\n",
       "      <th>zfxe2x80x99s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 6329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  02  06366  1  10  100  1000  100000  100000125000the  \\\n",
       "0    0  0   0      0  0   0    0     0       0                 \n",
       "1    0  0   0      2  0   0    0     0       0                 \n",
       "2    0  0   0      0  0   0    0     0       0                 \n",
       "3    0  0   0      0  0   0    0     0       0                 \n",
       "4    0  0   0      0  0   0    0     0       0                 \n",
       "..  .. ..  ..     .. ..  ..   ..    ..      ..                 \n",
       "421  0  0   0      1  0   0    0     0       0                 \n",
       "422  0  0   0      0  0   0    0     0       0                 \n",
       "423  0  0   0      0  0   0    0     0       0                 \n",
       "424  0  0   0      0  0   0    0     0       0                 \n",
       "425  0  0   0      0  0   0    0     0       0                 \n",
       "\n",
       "     1001schedulefulltimejob  ...  youwe  youwhat  youxe2x80x99ll  \\\n",
       "0    0                        ...  0      0        0                \n",
       "1    0                        ...  0      0        0                \n",
       "2    0                        ...  0      0        0                \n",
       "3    0                        ...  0      0        0                \n",
       "4    0                        ...  0      0        0                \n",
       "..  ..                        ... ..     ..       ..                \n",
       "421  0                        ...  0      0        1                \n",
       "422  0                        ...  0      0        0                \n",
       "423  0                        ...  0      0        0                \n",
       "424  0                        ...  0      0        0                \n",
       "425  0                        ...  0      0        0                \n",
       "\n",
       "     youxe2x80x99re  youxe2x80x99ve  youyou  yrs  zero  zf  zfxe2x80x99s  \n",
       "0    0               0               0       0    0     0   0             \n",
       "1    0               0               0       0    0     0   0             \n",
       "2    0               0               0       0    0     0   0             \n",
       "3    0               0               0       0    0     0   0             \n",
       "4    0               0               0       0    0     0   0             \n",
       "..  ..              ..              ..      ..   ..    ..  ..             \n",
       "421  0               0               0       0    0     0   0             \n",
       "422  0               0               0       0    0     0   0             \n",
       "423  0               0               0       0    1     0   0             \n",
       "424  0               0               0       0    0     0   0             \n",
       "425  0               0               0       0    0     0   0             \n",
       "\n",
       "[426 rows x 6329 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo1iH_UeY7_n"
   },
   "source": [
    "## 4) Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5LB00uyZKV5"
   },
   "outputs": [],
   "source": [
    "#construct count function\n",
    "from collections import Counter\n",
    "\n",
    "def count(docs):\n",
    "        \"\"\"This function returns the a data frame providing information of the most popular word counts for a given\n",
    "        corpus of documents\"\"\"\n",
    "        word_counts = Counter()\n",
    "        appears_in = Counter()\n",
    "        \n",
    "        total_docs = len(docs)\n",
    "\n",
    "        for doc in docs:\n",
    "            word_counts.update(doc)\n",
    "            appears_in.update(set(doc))\n",
    "\n",
    "        temp = zip(word_counts.keys(), word_counts.values())\n",
    "        \n",
    "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "        total = wc['count'].sum()\n",
    "\n",
    "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "        wc = wc.sort_values(by='rank')\n",
    "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "        t2 = zip(appears_in.keys(), appears_in.values())\n",
    "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "        wc = ac.merge(wc, on='word')\n",
    "\n",
    "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "        return wc.sort_values(by='rank')\n",
    "#Get counts of tokenized listings\n",
    "wc = count(tokenized_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>appears_in</th>\n",
       "      <th>count</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_total</th>\n",
       "      <th>cul_pct_total</th>\n",
       "      <th>appears_in_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datum</td>\n",
       "      <td>405</td>\n",
       "      <td>2584</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020811</td>\n",
       "      <td>0.020811</td>\n",
       "      <td>0.950704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>work</td>\n",
       "      <td>362</td>\n",
       "      <td>1355</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>0.031724</td>\n",
       "      <td>0.849765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>team</td>\n",
       "      <td>358</td>\n",
       "      <td>1171</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>0.041155</td>\n",
       "      <td>0.840376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>experience</td>\n",
       "      <td>368</td>\n",
       "      <td>1109</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.008932</td>\n",
       "      <td>0.050087</td>\n",
       "      <td>0.863850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>business</td>\n",
       "      <td>312</td>\n",
       "      <td>1073</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>0.058729</td>\n",
       "      <td>0.732394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15940</th>\n",
       "      <td>DomesticRegionalVirtual</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15953.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15939</th>\n",
       "      <td>Eligible</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15954.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15956</th>\n",
       "      <td>JobNoCerner</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15955.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15943</th>\n",
       "      <td>25000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15956.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15934</th>\n",
       "      <td>countriescernerxe2x80x99s</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15957.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15957 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word  appears_in  count     rank  pct_total  \\\n",
       "4      datum                      405         2584   1.0      0.020811    \n",
       "48     work                       362         1355   2.0      0.010913    \n",
       "319    team                       358         1171   3.0      0.009431    \n",
       "50     experience                 368         1109   4.0      0.008932    \n",
       "86     business                   312         1073   5.0      0.008642    \n",
       "...         ...                   ...          ...   ...           ...    \n",
       "15940  DomesticRegionalVirtual    1           1      15953.0  0.000008    \n",
       "15939  Eligible                   1           1      15954.0  0.000008    \n",
       "15956  JobNoCerner                1           1      15955.0  0.000008    \n",
       "15943  25000                      1           1      15956.0  0.000008    \n",
       "15934  countriescernerxe2x80x99s  1           1      15957.0  0.000008    \n",
       "\n",
       "       cul_pct_total  appears_in_pct  \n",
       "4      0.020811       0.950704        \n",
       "48     0.031724       0.849765        \n",
       "319    0.041155       0.840376        \n",
       "50     0.050087       0.863850        \n",
       "86     0.058729       0.732394        \n",
       "...         ...            ...        \n",
       "15940  0.999968       0.002347        \n",
       "15939  0.999976       0.002347        \n",
       "15956  0.999984       0.002347        \n",
       "15943  0.999992       0.002347        \n",
       "15934  1.000000       0.002347        \n",
       "\n",
       "[15957 rows x 7 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 100.0, 0.0, 100.0)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the most common words\n",
    "import squarify\n",
    "import matplotlib.pyplot as plt\n",
    "wc_top= wc[wc['rank'] <= 10]\n",
    "\n",
    "squarify.plot(sizes=wc_top['pct_total'], label=wc_top['word'], alpha=.8 )\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwFsTqrVZMYi"
   },
   "source": [
    "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gx2gZCbl5Np"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1 year</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <th>12 month</th>\n",
       "      <th>2</th>\n",
       "      <th>2 year</th>\n",
       "      <th>2019</th>\n",
       "      <th>25</th>\n",
       "      <th>...</th>\n",
       "      <th>year relevant</th>\n",
       "      <th>year work</th>\n",
       "      <th>yearsxe2x80x99</th>\n",
       "      <th>yearsxe2x80x99 experience</th>\n",
       "      <th>york</th>\n",
       "      <th>york city</th>\n",
       "      <th>youxe2x80x99ll</th>\n",
       "      <th>youxe2x80x99ll work</th>\n",
       "      <th>youxe2x80x99re</th>\n",
       "      <th>youxe2x80x99ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.135976</td>\n",
       "      <td>0.079861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1    1 year   10  100   12  12 month         2  2 year  2019   25  \\\n",
       "0  0.000000  0.000000  0.0  0.0  0.0  0.0       0.144098  0.0     0.0   0.0   \n",
       "1  0.135976  0.079861  0.0  0.0  0.0  0.0       0.000000  0.0     0.0   0.0   \n",
       "2  0.000000  0.000000  0.0  0.0  0.0  0.0       0.000000  0.0     0.0   0.0   \n",
       "3  0.000000  0.000000  0.0  0.0  0.0  0.0       0.000000  0.0     0.0   0.0   \n",
       "4  0.000000  0.000000  0.0  0.0  0.0  0.0       0.000000  0.0     0.0   0.0   \n",
       "\n",
       "   ...  year relevant  year work  yearsxe2x80x99  yearsxe2x80x99 experience  \\\n",
       "0  ...  0.0            0.0        0.0             0.0                         \n",
       "1  ...  0.0            0.0        0.0             0.0                         \n",
       "2  ...  0.0            0.0        0.0             0.0                         \n",
       "3  ...  0.0            0.0        0.0             0.0                         \n",
       "4  ...  0.0            0.0        0.0             0.0                         \n",
       "\n",
       "   york  york city  youxe2x80x99ll  youxe2x80x99ll work  youxe2x80x99re  \\\n",
       "0  0.0   0.0        0.0             0.0                  0.0              \n",
       "1  0.0   0.0        0.0             0.0                  0.0              \n",
       "2  0.0   0.0        0.0             0.0                  0.0              \n",
       "3  0.0   0.0        0.0             0.0                  0.0              \n",
       "4  0.0   0.0        0.0             0.0                  0.0              \n",
       "\n",
       "   youxe2x80x99ve  \n",
       "0  0.0             \n",
       "1  0.0             \n",
       "2  0.0             \n",
       "3  0.0             \n",
       "4  0.0             \n",
       "\n",
       "[5 rows x 2532 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want to convert a collection of raw documents to a matrix of TF-IDF features\n",
    "# Tunning Parameters\n",
    "\n",
    "# Instantiate vectorizer object\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        max_df=.97,\n",
    "                        min_df=.02,\n",
    "                        tokenizer=get_lemmas)\n",
    "data = df['description']\n",
    "# And create a vocabulary and get word counts per document\n",
    "\n",
    "# Similiar to fit_predict\n",
    "dtm2 = tfidf.fit_transform(data)\n",
    "\n",
    "\n",
    "# Print word counts\n",
    "\n",
    "# Get feature names to use as dataframe column headers\n",
    "dtm2 = pd.DataFrame(dtm2.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Fit on DTM\n",
    "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
    "nn.fit(dtm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 1.27364016, 1.27364016, 1.27727877, 1.28130477]]),\n",
       " array([[  0,  42, 138, 403, 366]]))"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify k nearest neighboors nearest to my query for the first document(observation)\n",
    "#Nearest Neighboor model is based on dtm2  (document term matrix created above)\n",
    "nn.kneighbors([dtm2.iloc[0].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make query based on my descritpion of my ideal data science job\n",
    "ideal_job = [\"\"\"My ideal job is one where I can apply software engineering,data wrangling, machine learning to solve problems dealing with setting up data infrastructures\"\"\"]\n",
    "new = tfidf.transform(ideal_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2532 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.28476157, 1.30351228, 1.30739224, 1.30748933, 1.30952434]]),\n",
       " array([[276, 115, 307, 399, 130]]))"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.kneighbors(new.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bFacebooks mission is to give people the power to build community and bring the world closer together Through our family of apps and services were building a different kind of company that connects billions of people around the world gives them ways to share what matters most to them and helps bring people closer together Whether were creating new products or helping a small business expand its reach people at Facebook are builders at heart Our global teams are constantly iterating solving problems and working together to empower people around the world to build community and connect in meaningful ways Together we can help people build stronger communities xe2x80x94 were just getting startedThe Infrastructure Strategy group is responsible for the strategic analysis to support and enable the continued growth critical to Facebookxe2x80x99s infrastructure organization The ideal candidate will be passionate about Facebook have strong analytical and modeling aptitude and has experience using data to drive cost effective decision makingRESPONSIBILITIESLeverage data and business principles to solve largescale web mobile and data infrastructure problemsWork crossfunctionally to define problem statements collect data build analytical models and make recommendationsBuild and maintain data driven optimization models experiments forecasting algorithms and machine learning modelsLeverage tools like Python R Hadoop  SQL to drive efficient analyticsCommunicate final recommendations and drive decision makingMINIMUM QUALIFICATIONSDegree in quantitative field eg Computer Science Engineering Mathematics Statistics Operations Research or other related field2 years of industry or graduate research experience solving analytical problems and building models using quantitative statistical or machine learning approachesExperience with Machine Learning Statistics or other data analysis tools and techniquesExperience performing data extraction cleaning analysis and presentation for medium to large datasetsExperience with at least one programming language ie Python R Java or CExperience writing SQL queriesExperience with scientific computing and analysis packages such as NumPy SciPy Pandas Scikitlearn dplyr or ggplot2Experience with statistics methods such as forecasting time series hypothesis testing classification clustering or regression analysisExperience with data visualization libraries such as Matplotlib Pyplot ggplot2Experience with machine learning libraries and packages such as PyTorch Caffe2 TensorFlow Keras or TheanoPREFERRED QUALIFICATIONSAdvanced degree Masterxe2x80x99s or PhD in quantitative fieldExperience working with distributed computing tools Hadoop Hive Spark etcProficiency in algorithmic complexity'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Most relevant result\n",
    "data[276]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDfTWceoRkH"
   },
   "source": [
    "## Stretch Goals\n",
    "\n",
    " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
    " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
    " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
    " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
    " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_BOW_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

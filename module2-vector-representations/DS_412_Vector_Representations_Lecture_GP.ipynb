{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM-KfuCGdQ_9"
      },
      "source": [
        "*Unit 4, Sprint 1, Module 2*\n",
        "\n",
        "---\n",
        "\n",
        "# Vector Representations (Prepare)\n",
        "\n",
        "\n",
        "As we learned yesterday, machines cannot interpret raw text. We need to transform that text into something we/machines can more readily analyze. Yesterday, we did simple counts of counts to summarize the content of Amazon reviews. Today, we'll extend those concepts to talk about vector representations such as Bag of Words (BoW) and word embedding models. We'll use those representations for search, visualization, and prepare for our classification day tomorrow.\n",
        "\n",
        "Processing text data to prepare it for machine learning models often means translating the information from documents into a numerical format. Bag-of-Words approaches (sometimes referred to as Frequency-Based word embeddings) accomplish this by \"vectorizing\" tokenized documents. This is done by representing each document as a row in a DataFrame and creating a column for each unique word in the corpora (group of documents). The presence or lack of a given word in a document is then represented either as a raw count of how many times a given word appears in a document (CountVectorizer) or as that word's TF-IDF score (TfidfVectorizer).\n",
        "\n",
        "On the python side, we will be focusing on `sklearn` and `spacy` today.  \n",
        "\n",
        "## Case Study\n",
        "\n",
        "We're going to pretend we're on the datascience team at the BBC. We want to recommend articles to visitors to on the BBC website based on the article they just read.\n",
        "\n",
        "[BBC data set](http://mlg.ucd.ie/datasets/bbc.html)<br>\n",
        "For today's exercise, we have downloaded the \"raw text files\" from the \"BBC\" data set and focused on the tech articles only\n",
        "\n",
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Represent a document as a vector\n",
        "* <a href=\"#p2\">Part 2</a>: Query Documents by Similarity\n",
        "* <a href=\"#p3\">Part 3</a>: Apply word embedding models to create document vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxtT9pdddRAB"
      },
      "source": [
        "### Let's start with an analogy\n",
        "\n",
        "![](https://peterbeshai.com/static/d9c3868cc1becd7648da453597a5d616/36dbb/DER_cube.jpg)\n",
        "\n",
        "Pretend that the objecting floating in the room is our raw text dataset (i.e. a collection of documents).\n",
        "\n",
        "A vectorizer is a mathematical transformation that takes our raw text data and transforms it into a numerical representation (i.e. numbers inside of vectors).\n",
        "\n",
        "Depending on which vectorizer you use, you will be capturing some of the information encoded in the text but not other information.\n",
        "\n",
        "So, as the analogy goes, depending which side of the floating object you stand and shine a light from, you will only see a portion of the information that exists in your text data set. Apply one vectorizer and you'll get word counts but not the contextual meaning; apply another vectorizer and you'll get the contextual meaning of the words but not the counts.\n",
        "\n",
        "**Take Away:** Understand the benefits and limitations of using each of the vectorizers that we'll be learning today.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmTC1bWMBz02"
      },
      "source": [
        "# 0. Colab notebook setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si_FTZzkeGFD"
      },
      "source": [
        "##0.1 Get `spacy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMfQKVrneA38"
      },
      "source": [
        "%%time\n",
        "# You'll use en_core_web_sm for the sprint challenge due memory constraints on Codegrader\n",
        "#!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Locally (or on colab) let's use en_core_web_lg\n",
        "!python -m spacy download en_core_web_md # Can do lg, takes awhile\n",
        "# Also on Colab, need to restart runtime after this step!\n",
        "#      or else Colab won't find spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRKrvC4feP15"
      },
      "source": [
        "##0.2 Restart the runtime!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eba3NjeAO-Ha"
      },
      "source": [
        "## 0.3 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XVf_1L3dRAD"
      },
      "source": [
        "\"\"\" Import Statements \"\"\"\n",
        "\n",
        "# Classics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55skIZ7eidH7"
      },
      "source": [
        "##0.4 Clone the git repo\n",
        "so we can access the files in the `/data` folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpgi5SBuePAD"
      },
      "source": [
        "!git clone https://github.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD4Kv86TjEfJ"
      },
      "source": [
        "## 0.5 Get the BBC tech articles data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v8uWdQRBjF7"
      },
      "source": [
        "Helper function to read articles in `.txt` files in a specified directory and gather them into a big list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m0vfCnqdRAI"
      },
      "source": [
        "import os\n",
        "\n",
        "def gather_data(filefolder):\n",
        "    \"\"\" Produces List of Documents from a Directory\n",
        "\n",
        "    filefolder (str): a path of .txt files\n",
        "\n",
        "    returns list of strings\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "\n",
        "    files = os.listdir(filefolder) # Causes variation across machines\n",
        "\n",
        "    for filename in files:\n",
        "\n",
        "        path = os.path.join(filefolder, filename)\n",
        "\n",
        "        if  path[-3:] == 'txt': # os ~endswith('txt')\n",
        "            with open(path, 'rb') as f:\n",
        "                data.append(f.read())\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSH2TnEnBsK7"
      },
      "source": [
        "Read the articles into a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsYq8x6dRAJ"
      },
      "source": [
        "# locate and get path to the /data folder, using colab's file browser\n",
        "data_path = '/content/DS-Unit-4-Sprint-1-NLP/module2-vector-representations/data'\n",
        "data = gather_data(data_path)\n",
        "print(f'Number of Articles: {len(data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gx9kYDLCGtN"
      },
      "source": [
        "Articles turn out to be \"byte strings\" so we need to decode them to strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC8NUCwhCY3Z"
      },
      "source": [
        "print(data[20])\n",
        "print(type(data[20]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubER5c_yPLMG"
      },
      "source": [
        "# decode bytestrings in a corpus to strings\n",
        "# takes a list of documents, i.e a list of strings as input\n",
        "articles = []\n",
        "for article in data:\n",
        "  article = article.decode(\"utf-8\")\n",
        "  articles.append(article)\n",
        "print(len(articles))\n",
        "\n",
        "# Preview first article and check data type again\n",
        "data = articles\n",
        "print(data[0])\n",
        "print(type(data[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CF_285ECuXV"
      },
      "source": [
        "Make a `spacy` tokenizer, as in Module 1<br>\n",
        "Question: why did we use [`.strip()`](https://python-reference.readthedocs.io/en/latest/docs/str/strip.html)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlnxqpI2dRAQ"
      },
      "source": [
        "def tokenize(document):\n",
        "    \"\"\"\n",
        "    Takes a doc (text string) and returns a list of tokens in the form of lemmas.\n",
        "    Filters out Stop words, punctuation, and leading/trailing spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    doc = nlp(document)\n",
        "    # last \"and\" statement is to remove token lemmas that are empty strings\n",
        "    lemma_list = [token.lemma_.lower().strip() for token in doc if (not token.is_stop)\n",
        "                                                                and (not token.is_punct )\n",
        "                                                                and (token.lemma_.strip()!=\"\")\n",
        "                                                                and (len(token.lemma_.strip())>1)]\n",
        "    return lemma_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our tokenizer\n",
        "tokenize('<Insert sentence here>')"
      ],
      "metadata": {
        "id": "NpDrlDB8SuGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKox9ksM2RWM"
      },
      "source": [
        "data[20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Zl0PeDOigp"
      },
      "source": [
        "tokenize(data[20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvGZ-eiUdRAB"
      },
      "source": [
        "# 1. Represent a document as a vector (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSUmQeX5dRAC"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this section, we are going to create Document Term Matrices (DTM). <br>\n",
        "In the DTM, each row represents a document. The columns correspond to the vocabulary words of the entire corpus of documents. <br>\n",
        "Today, we'll learn three methods of computing the value in each cell. <br>\n",
        "* `CountVectorizer` from `sklearn` provides a basic implementation: counts of appearances of words. You could also ignore multiple occurrences of words in a document, and just keep track of whether or not a word occurs in the document via a boolean value.<br>\n",
        "* `TfidfVectorizer` is a more advanced implementation that keeps track of *term-frequency inverse-document frequency* (TF-IDF) instead of integer counts.\n",
        "\n",
        "* Word embeddings, an even more sophisticated method that helps solve the problem of accounting for the context of a word.\n",
        "\n",
        "**Discussion:** Don't we lose all the context and grammar if we do this? So Why does it work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "TQQRch8mdRAD"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVIflCt1dRAF"
      },
      "source": [
        "**Warm Up (_3 Minutes_)**\n",
        "\n",
        "Extract the tokens from this sentence using Spacy. Text is from [OpenAI](https://openai.com/blog/better-language-models/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg63BJ6-dRAG"
      },
      "source": [
        "text = \"\"\"\n",
        "GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality,\n",
        "where we prime the model with an input and have it generate a lengthy continuation. In addition, GPT-2 outperforms other language models\n",
        "trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language\n",
        "tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text,\n",
        "using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks\n",
        "can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.\"\"\"\n",
        "\n",
        "print(f'Number of Words: {len(text.split())}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9rHYGcKRX-n"
      },
      "source": [
        "tokens = tokenize(text)\n",
        "print(f'Number of Words: {len(tokens)}')\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kJlWC4BdRAK"
      },
      "source": [
        "-----\n",
        "# 1.1 `CountVectorizer`\n",
        "\n",
        "![](https://images4.programmersought.com/947/0a/0acb9279d17a1631bcfb154583cca443.JPEG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c6a74f21ed3917ee",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "gRM5YA8KdRAK"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# list of text documents\n",
        "\n",
        "# create the transformer\n",
        "\n",
        "# build vocab\n",
        "\n",
        "# transform text\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# a corpus is a collection of text documents\n",
        "corpus = [\"We created a new dataset which emphasizes diversity of content, by scraping content from the Internet.\",\n",
        "          \" In order to preserve document quality, we used only pages which have been curated/filtered by humans—specifically, we used outbound links from Reddit which received at least 3 karma.\",\n",
        "          \" This can be thought of as a heuristic indicator for whether other users found the link interesting (whether educational or funny), leading to higher data quality than other similar datasets, such as CommonCrawl.\"]\n",
        "\n",
        "# create the transformer\n",
        "\n",
        "\n",
        "# build vocabulary\n",
        "\n",
        "\n",
        "# transform text to create the document-term matrix\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-93323028a4bfd7e7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "scrolled": true,
        "id": "YGKln3xLdRAL"
      },
      "source": [
        "# explore the document-term matrix\n",
        "\n",
        "# Get feature names\n",
        "\n",
        "# print out raw text for comparison with  indexing\n",
        "\n",
        "# show indexing of token count\n",
        "\n",
        "# Dealing with Sparse Matrix\n",
        "\n",
        "# Get Word Counts for each document\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "# Get feature names -- this is the \"vocablulary\" for the corpus\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcTytc3-kKIm"
      },
      "source": [
        "# explore the document-term matrix and understand it's structure\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99utB63ckHZO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBq6oeR-FhIW"
      },
      "source": [
        "# Get Word Counts for each document and format as a dataframe\n",
        "\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg3lw6qok5wR"
      },
      "source": [
        "dtm_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJWsLvkXdRAL"
      },
      "source": [
        "**Three Minute Challenge:**\n",
        "* Apply CountVectorizer to our BBC Data\n",
        "* Store results in a dataframe called `dtm`\n",
        "* Extra Challenge - Try to Customize CountVectorizer with Spacy Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3a7cd500efe1978b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "0LQrT4qydRAL"
      },
      "source": [
        "# Apply CountVectorizer to our Data\n",
        "# Use custom Spacy Vectorizer\n",
        "# BBC articles in `data` variable\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Learn our Vocab\n",
        "# Get sparse dtm (i.e. transform the data)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "# Learn our Vocab\n",
        "\n",
        "\n",
        "# Transform the BBC news articles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6JBj-YUdRAM"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "doc_len = [len(doc.split()) for doc in data]\n",
        "sns.distplot(doc_len);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYgUj_WcdRAN"
      },
      "source": [
        "#1.2  `TfidfVectorizer`\n",
        "\n",
        "## Term Frequency - Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "![](https://miro.medium.com/max/1404/1*mu6G-cBmWlENS4pWHEnGcg@2x.jpeg)\n",
        "\n",
        "\n",
        "**Term Frequency**: $\\text{tf}_{i,j}$  is the number of times term $i$ appeared in document $j$\n",
        "\n",
        "**Inverse Document Frequency:** A weight penalty for terms that exist in a high fraction of documents.\n",
        "\n",
        "The purpose of TF-IDF is to find what is **unique** to each document. Because of this we will penalize the term frequencies of words that are common across all documents. This effectively upweights terms that are rarer across documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfzMqqbrdRAN"
      },
      "source": [
        "#1.2  `TfidfVectorizer`\n",
        "\n",
        "## Term Frequency - Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "![](https://miro.medium.com/max/1404/1*mu6G-cBmWlENS4pWHEnGcg@2x.jpeg)\n",
        "\n",
        "\n",
        "**Term Frequency**: $\\text{tf}_{i,j}$  is the number of times term $i$ appeared in document $j$\n",
        "\n",
        "**Inverse Document Frequency:** A weight penalty for terms that exist in a high fraction of documents.\n",
        "\n",
        "The purpose of TF-IDF is to find what is **unique** to each document. Because of this we will penalize the term frequencies of words that are common across all documents. This effectively upweights terms that are rarer across documents.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/Logarithm_plots.png\" width=\"700\" height=\"400\" />\n",
        "\n",
        "It's useful to reference both the algebraic and geometric representations of a single mathematical ideal whenever possible in order to build the fullest understanding possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uysxF1NdRAN"
      },
      "source": [
        "The IDF portion of the TF-IDF equation has been coded up below so that we can play around with the values and get a better understanding of how this portion of the equation works.\n",
        "\n",
        "NOTE: There are other ways to construct the equation for the IDF term; different constructions of the equation serve different purposes and which you ultimately use simply depends on your problem/task. You can check out the [wikipedia article on tfidf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
        "\n",
        "In practice, you usually don't even have to think about the mathematical formulation of the IDF term. You simply import the tfidf vectorizer and use its api. The rare exception to this would if you are working a very particular kind of problem where the open source implementation of tfidf doesn't suit your needs and so then you might consider creating your own equation or using a different one that you read about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk_AVqh3dRAN"
      },
      "source": [
        "# inverse document frequency score\n",
        "# the plus ones are constants that shift the around the baseline value\n",
        "def idf(n, df):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    n: int\n",
        "        num of docs in corpus\n",
        "\n",
        "    df: int\n",
        "        num of docs that term t (i.e. a token) appears in\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    inverse docuemnt frequency: float\n",
        "    \"\"\"\n",
        "    return np.log( n/ df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4e40eb25d7faed91",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1uh6Ab7FdRAO"
      },
      "source": [
        "n = 100                 # num of docs in corpus\n",
        "df_range = 100\n",
        "IDF = []\n",
        "DF = []\n",
        "for df in range(1, df_range + 1):\n",
        "    idf_score = idf(n, df)\n",
        "    IDF.append(idf_score)\n",
        "    DF.append(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVAUfwlddRAO"
      },
      "source": [
        "plt.figure()\n",
        "plt.title(\"Inverse Document Frequency Score vs. Document Frequency of Token\")\n",
        "plt.ylabel(\"Inverse Document Frequency Score\")\n",
        "plt.xlabel(\"Document Frequency of Token\")\n",
        "plt.grid()\n",
        "plt.plot(DF, IDF);\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Inverse Document Frequency Score vs. Document Frequency of Token\")\n",
        "plt.ylabel(\"Inverse Document Frequency Score\")\n",
        "plt.xlabel(\"Document Frequency of Token\")\n",
        "plt.xlim(95,100)\n",
        "plt.ylim(0,0.10)\n",
        "plt.grid()\n",
        "plt.plot(DF, IDF);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bed0fb42b084357d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "G7dv3fH2dRAO"
      },
      "source": [
        "# let's work through a couple of examples to help build intuition\n",
        "# imagine the token we are considering is \"purchase\", which rarely appears\n",
        "# imagine the token that we are considering is \"the\", which appears often\n",
        "\n",
        "n = 100\n",
        "\n",
        "# imagine the token we are considering is \"purchase\"\n",
        "tf = 1\n",
        "df = 20\n",
        "\n",
        "tfidf_score = tf * idf(n, df)\n",
        "print(f'purchase: {tfidf_score:.2f}')\n",
        "\n",
        "# imagine the token that we are considering is \"the\"\n",
        "tf = 50\n",
        "df = 95\n",
        "\n",
        "tfidf_score = tf * idf(n, df)\n",
        "print(f'the: {tfidf_score:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQbFzVXAdRAP"
      },
      "source": [
        "Now that we have built some intuition on how TFIDF works, let's use `sklearn`'s `TfidfVectorizer` to vectorize our dataset. <br>\n",
        "Note that `TfidfVectorizer` is used with the same syntax as `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4ccfa3ef2d0404fc",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Lm6FsqHOdRAP"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Instantiate vectorizer object\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "# 2. Create a vocabulary and get word counts per document\n",
        "dtm = tfidf.fit_transform(data)\n",
        "\n",
        "# 3. View term-document matrix as DataFrame\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names_out())\n",
        "print(dtm.shape)\n",
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_VFnOHrMMtT"
      },
      "source": [
        "Now let's get a little fancier with the `TfidfVEctorizer`: we'll use the keywords `ngram_range`, `max_df`, `min_df` and `tokenizer` to accomplish the following:\n",
        "- Build a vocabulary from unigrams and bigrams\n",
        "- Do \"statistical trimming\" by ignoring all terms with document frequency higher than `max_df` and all terms with document frequency lower than `min_df`\n",
        "- Use the `spaCy` tokenizer that we built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0667a7a2ebea2224",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "cKB5QWy-dRAQ"
      },
      "source": [
        "%%time\n",
        "# Tuning the Parameters of our Vectorizer\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "# Instantiate vectorizer object (ngram_range includes unigrams and bigrams)\n",
        "tfidf = TfidfVectorizer(...)\n",
        "\n",
        "# Create a vocabulary and get word counts per document\n",
        "dtm = tfidf.fit_transform(data) # Similiar to fit_predict -- produces a sparse matrix\n",
        "\n",
        "# Get feature names to use as dataframe column headers\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names_out())\n",
        "print(dtm.shape)\n",
        "\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWJZl78z__ay"
      },
      "source": [
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTMPm-hbdRAQ"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "During this module's project assignment, you will transform data science job listings to vector representations for analysis downstream."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oddqm9omdRAR"
      },
      "source": [
        "# 2. Query Documents by Similarity (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myAGpeGOdRAR"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Have you ever considered how a search bar works in Google? You may just think that search bars simply match your input text against the documents. While there are many different mechanisms for the 'match', one of the most classic is to search by similarity. We will apply two measures of similarity between a document and the input query document: the **Cosine Similarity** between the documents and the **Euclidean distance** between the documents. <br>\n",
        "\n",
        "Cosine Similarity is [more compute intensive](https://stackoverflow.com/questions/28917985/why-cosine-distance-is-much-slower-than-using-euclidean-distance-with-dbscan-alg) than Euclidean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "GG_E_NYfdRAR"
      },
      "source": [
        "## Follow Along\n",
        "*Cosine similarity* and *nearest-neighbor distance* provide two measures that can be used to gauge the similarity of one word to another. <br>\n",
        "We develop these concepts in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_vR2dvwdRAS"
      },
      "source": [
        "### 2.1 Cosine Similarity\n",
        "The [`cosine_simlarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) between two feature vectors is the dot product of the *normalized* feature vectors. <br>To *normalize* a vector, you simply divide it by its length: thus<br><br>\n",
        "$\\text{cosine_similarity}(\\textbf{u},\\textbf{v})= \\frac{\\textbf{u}}{\\left\\|\\textbf{u}\\right\\|} \\cdot \\frac{\\textbf{v}}{\\left\\|\\textbf{v}\\right\\|}$, <br><br>\n",
        "where $\\left\\|\\textbf{u}\\right\\|$ is the length of the vector $\\textbf{u}$<br><br>\n",
        "\n",
        "$\\textbf{cosine_similarity}$ is in the interval $[0, 1]$, <br><br>\n",
        "The larger the $\\textbf{cosine_similarity}$, the more similar the words are, and vice-versa.<br><br>\n",
        "\n",
        "`cosine_similarity` takes as input the document-term matrix <br>\n",
        "and creates a **similarity matrix** in which the rows and columns represent documents <br>\n",
        "and entry ($i, j$) is the cosine similarity between documents $i$ and $j$.<br><br>\n",
        "\n",
        "Let's compute the **similarity matrix** for the BBC tech articles corpus<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-76ce78f9798d38bc",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-PunUo9xdRAS"
      },
      "source": [
        "%%time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate Distance of TF-IDF Vectors\n",
        "\n",
        "# Turn it into a DataFrame\n",
        "\n",
        "# Our Similarity Matrix is ? size\n",
        "\n",
        "# Each row is the similarity of one document to all other documents (including itself)\n",
        "\n",
        "# Grab the row and pick off the indicies of the most/least similar docs\n",
        "\n",
        "# Calculate Distance of TF-IDF Vectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# similarity matrix\n",
        "dist_matrix = cosine_similarity(dtm)\n",
        "\n",
        "# Turn it into a DataFrame\n",
        "df = pd.DataFrame(dist_matrix)\n",
        "\n",
        "# Size of the Similarity Matrix\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMaFFxxV9k6e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDtkkqU8Qx25"
      },
      "source": [
        "# Each row is the similarity of one document to all other documents (including itself)\n",
        "df.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFV_5FzLijNo"
      },
      "source": [
        "### Find the most similar documents to the first document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW0zaZkFQtgS"
      },
      "source": [
        "### BEGIN SOLUTION\n",
        "# Grab the first row and return the indices of the 5 most similar docs\n",
        "\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msStkSPqdRAS"
      },
      "source": [
        "### 2.2 K Nearest Neighbors Search (kNN)\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/753/0*jqxx3-dJqFjXD6FA\" width=\"600\" height=\"400\" />\n",
        "\n",
        "\n",
        "Nearest Neighbor models are distance based algorithms. They store your training set in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD4Oru_DlAhu"
      },
      "source": [
        "### `scikit-learn`'s `NearestNeighbors` class\n",
        "computes distances between all pairs of data points, <br>\n",
        "and returns the result in the form of a similarity matrix. <br>\n",
        "Entry $(i,j)$ in the similarity matrix records the similarity of data points $i$ and $j$.<br>\n",
        "\n",
        "Below is an example using `NearestNeighbors` to find the most similar documents to a given documentL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-eac83ba5d76f6bf2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "OaF7cC7jdRAS"
      },
      "source": [
        "%%time\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Instantiate the nearest neighbors model\n",
        "nn = NearestNeighbors()\n",
        "\n",
        "# fit to our document term matrix\n",
        "nn.fit(dtm.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPbW_1sum9NP"
      },
      "source": [
        "Find the 5 most similar documents to a query document; here we will choose the first document as our query document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krvdbifUpi4X"
      },
      "source": [
        "# sample a doc from a dtm to use as our query point\n",
        "doc_index = 0\n",
        "query_doc = dtm.values[0][None,:] # re-cast as a row vector\n",
        "\n",
        "# Query Using the kneighbors method\n",
        "# NOTE: nn counts the original document as one of the neighbors, so if we want 5 nearest neighbors, we should set n_neightbors=6\n",
        "neigh_dist, neigh_index = nn.kneighbors(query_doc,n_neighbors=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SUrN_YrvPsK"
      },
      "source": [
        "# nearest neighbor search returns the same documents as cosine similarity!\n",
        "print(neigh_index)\n",
        "print(neigh_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDM_NrH7IBr8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7jzq5UdVWIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which words do these two documents have in common?"
      ],
      "metadata": {
        "id": "GJvImw9sYxQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shared_words(s1, s2):\n",
        "  res = []\n",
        "  l_s1, l_s2 = set(s1.split()), set(s2.split())\n",
        "  for ss1 in l_s1:\n",
        "    if ss1 in l_s2: res.append(ss1)\n",
        "  return res\n",
        "\n",
        "doc1_tokens = tokenize(data[0])\n",
        "doc2_tokens = tokenize(data[...])\n",
        "shared_words(' '.join(doc1_tokens), ' '.join(doc2_tokens))"
      ],
      "metadata": {
        "id": "zlxQ968QYtrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhG-KESidGIn"
      },
      "source": [
        "### Exercise:\n",
        "Search the BBC dataset for documents most similar to a random query doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouWYmCawdRAT"
      },
      "source": [
        "rndm_tech_article = [ \"\"\"\n",
        "Blockchain technology encompasses so much more than just cryptocurrencies these days. And while these currencies are by far blockchain’s most famous use case, the potential for blockchain far, far exceeds a straightforward transfer of value.\n",
        "The gaming industry, for example, has always been fairly controversial due to a range of issues. Many players feel particularly worried about the protection of their data, fraudulent activities, high fees, and most of all, unfair and hidden odds.\n",
        "In these cases, P2P doesn’t just mean peer-to-peer; it means player-to-player. As with the other P2P industries that blockchain is disrupting, such as FinTech, there are already plenty of ways in which blockchain technology is being applied to resolve these issues and many more of the challenges faced by the gaming industry. Here are just a few.\n",
        "Reducing fraud\n",
        "The gaming industry suffers a lot from online fraud. One of the main advantages of blockchain technology is that it enables highly secure data encryption. What’s more, all of it is entirely accessible and transparent to the player. In other words, blockchain is a for game changer for venues such as online casinos, lotteries, and virtually anything which relies on random number generation.\n",
        "On top of this, hackers will have particularly hard time, if it is even possible, to destroy a decentralized blockchain network, making sure gamer data stays safe. This is an inherent feature of distributed ledger technology whereby nodes in the ledger maintain the distributed databases in a shared manner, and each node has the complete information in the entire database.\n",
        "\"\"\"]\n",
        "\n",
        "\n",
        "# transform the rndm_tech_article to get its document vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use k Nearest Neighbors to find the most similar documents in our document term matrix to the query\n"
      ],
      "metadata": {
        "id": "d1Z_yaXfVpOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the most similar documents\n"
      ],
      "metadata": {
        "id": "CQrZ5URfVqU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaSuFg0ldRAT"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In the module project assignment, you will apply one of these search techniques to retrieve documents related to a query document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ugKeF8odRAT"
      },
      "source": [
        "# 3. Apply word embedding models to create document vectors (Learn)\n",
        "<a id=\"p3\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5wmBPMUdRAU"
      },
      "source": [
        "## Overview\n",
        "### Bag of Words (BoW) models lose information about context\n",
        "\n",
        "One of the limitations of Bag-of-Words approaches is that any information about the context surrounding that word is lost. This also means that with bag-of-words approaches often the only tools that we have for identifying words with similar usage or meaning and subsequently consolidating them into a single vector is through the processes of stemming and lemmatization which tend to be quite limited at consolidating words unless the two words are very close in their spelling or in their root parts-of-speech.\n",
        "\n",
        "### Embedding approaches preserve more textual context\n",
        "Word2Vec is an increasingly popular word embedding technique. Like Bag-of-words it learns a real-value vector representation for a predefined fixed-size vocabulary that is generated from a corpus of text. However, in contrast to BoW, Word2Vec approaches are much more capable of accounting for textual context, and are better at discovering words with similar meanings or usages (semantic or syntactic similarity).\n",
        "\n",
        "##3.1 Word2Vec Intuition\n",
        "Reference: [Deep NLP: Word Vectors with Word2Vec](https://medium.com/deep-learning-demystified/deep-nlp-word-vectors-with-word2vec-d62cb29b40b3)\n",
        "### The Distribution Hypothesis\n",
        "\n",
        "In order to understand how Word2Vec preserves textual context we have to understand what's called the [Distribution Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics). The Distribution Hypothesis operates under the assumption that words that have similar contexts will have similar meanings. Practically speaking, this means that if two words are found to have similar words both to the right and to the left of them throughout the corpora then those words have the same context and are assumed to have the same meaning.\n",
        "\n",
        "> \"You shall know a word by the company it keeps\" - John Firth\n",
        "\n",
        "This means that we let the usage of a word define its meaning and its \"similarity\" to other words. In the following example, which words would you say have a similar meaning?\n",
        "\n",
        "**Sentence 1**: Traffic was light today\n",
        "\n",
        "**Sentence 2**: Traffic was heavy yesterday\n",
        "\n",
        "**Sentence 3**: Prediction is that traffic will be smooth-flowing tomorrow since it is a national holiday\n",
        "\n",
        "What words in the above sentences seem to have a similar meaning if all you knew about them was the context in which they appeared above?\n",
        "\n",
        "Lets take a look at how this might work in action, the following example is simplified, but will give you an idea of the intuition for how this works.\n",
        "\n",
        "#### Corpora:\n",
        "\n",
        "1) \"It was the sunniest of days.\"\n",
        "\n",
        "2) \"It was the rainiest of days.\"\n",
        "\n",
        "#### Vocabulary:\n",
        "\n",
        "{\"it\": 1, \"was\": 2, \"the\": 3, \"of\": 4, \"days\": 5, \"sunniest\": 6, \"rainiest\": 7}\n",
        "\n",
        "### Vectorization\n",
        "\n",
        "|       doc   | START_was | it_the | was_sunniest | the_of | sunniest_days | of_it | days_was | it_the | was_rainiest | rainiest_days | of_END |\n",
        "|----------|-----------|--------|--------------|--------|---------------|-------|----------|--------|-------------|--------------|--------|\n",
        "| it       | 1         | 0      | 0            | 0      | 0             | 0     | 1        | 0      | 0           | 0            | 0      |\n",
        "| was      | 0         | 1      | 0            | 0      | 0             | 0     | 0        | 1      | 0           | 0            | 0      |\n",
        "| the      | 0         | 0      | 1            | 0      | 0             | 0     | 0        | 0      | 1           | 0            | 0      |\n",
        "| sunniest | 0         | 0      | 0            | 1      | 0             | 0     | 0        | 0      | 0           | 0            | 0      |\n",
        "| of       | 0         | 0      | 0            | 0      | 1             | 0     | 0        | 0      | 0           | 1            | 0      |\n",
        "| days     | 0         | 0      | 0            | 0      | 0             | 0     | 0        | 0      | 0           | 0            | 1      |\n",
        "| rainiest  | 0         | 0      | 0            | 1      | 0             | 0     | 0        | 0      | 0           | 0            | 0      |\n",
        "\n",
        "Each column vector represents the word's context -in this case defined by the words to the left and right of the center word. How far we look to the left and right of a given word is referred to as our \"window of context.\" Each row vector represents the the different usages of a given word. Word2Vec can consider a larger context than only words that are immediately to the left and right of a given word, but we're going to keep our window of context small for this example. What's most important is that this vectorization has translated our documents from a text representation to a numeric one in a way that preserves information about the underlying context.\n",
        "\n",
        "We can see that words that have a similar context will have similar row-vector representations, but before looking that more in-depth, lets simplify our vectorization slightly. You'll notice that we're repeating the column-vector \"it_the\" twice. Lets combine those into a single vector by adding them element-wise.\n",
        "\n",
        "|       *   | START_was | it_the | was_sunniest | the_of | sunniest_days | of_it | days_was | was_rainiest | rainiest_days | of_END |\n",
        "|----------|-----------|--------|--------------|--------|---------------|-------|----------|-------------|--------------|--------|\n",
        "| it       | 1         | 0      | 0            | 0      | 0             | 0     | 1        | 0           | 0            | 0      |\n",
        "| was      | 0         | 2      | 0            | 0      | 0             | 0     | 0        | 0           | 0            | 0      |\n",
        "| the      | 0         | 0      | 1            | 0      | 0             | 0     | 0        | 1           | 0            | 0      |\n",
        "| sunniest | 0         | 0      | 0            | 1      | 0             | 0     | 0        | 0           | 0            | 0      |\n",
        "| of       | 0         | 0      | 0            | 0      | 1             | 0     | 0        | 0           | 1            | 0      |\n",
        "| days     | 0         | 0      | 0            | 0      | 0             | 0     | 0        | 0           | 0            | 1      |\n",
        "| rainiest  | 0         | 0      | 0            | 1      | 0             | 0     | 0        | 0           | 0            | 0      |\n",
        "\n",
        "Now, can you spot which words have a similar row-vector representation? Hint: Look for values that are repeated in a given column. Each column represents the context that word was found in. If there are multiple words that share a context then those words are understood to have a closer meaning with each other than with other words in the text.\n",
        "\n",
        "Lets look specifically at the words sunniest and rainiest. You'll notice that these two words have exactly the same 10-dimensional vector representation. Based on this very small corpora of text we would conclude that these two words have relatd meanings because they share the same usage. Is this a good assumption? Well, they are both referring to the weather outside so that's better than nothing. You could imagine that as our corpora grows larger we will be exposed a greater number of contexts and the Distribution Hypothesis assumption will improve.\n",
        "\n",
        "##3.2 Word2Vec Variants\n",
        "\n",
        "###3.2.1 Skip-Gram\n",
        "\n",
        "The Skip-Gram method predicts the neighbors’ of a word given a center word. In the skip-gram model, we take a center word and a window of context (neighbors) words to train the model and then predict context words out to some window size for each center word.\n",
        "\n",
        "This notion of “context” or “neighboring” words is best described by considering a center word and a window of words around it.\n",
        "\n",
        "For example, if we consider the sentence **“The speedy Porsche drove past the elegant Rolls-Royce”** and a window size of 2, we’d have the following pairs for the skip-gram model:\n",
        "\n",
        "**Text:**\n",
        "**The**\tspeedy\tPorsche\tdrove\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (the, speedy), (the, Porsche)\n",
        "\n",
        "**Text:**\n",
        "The\t**speedy**\tPorsche\tdrove\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (speedy, the), (speedy, Porsche), (speedy, drove)\n",
        "\n",
        "**Text:**\n",
        "The\tspeedy\t**Porsche**\tdrove\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (Porsche, the), (Porsche, speedy), (Porsche, drove), (Porsche, past)\n",
        "\n",
        "**Text:**\n",
        "The\tspeedy\tPorsche\t**drove**\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (drove, speedy), (drove, Porsche), (drove, past), (drove, the)\n",
        "\n",
        "The **Skip-gram model** is going to output a probability distribution i.e. the probability of a word appearing in context given a center word and we are going to select the vector representation that maximizes the probability.\n",
        "\n",
        "With CountVectorizer and TF-IDF the best we could do for context was to look at common bi-grams and tri-grams (n-grams). Well, skip-grams go far beyond that and give our model much stronger contextual information.\n",
        "\n",
        "![alt text](https://www.dropbox.com/s/c7mwy6dk9k99bgh/Image%202%20-%20SkipGrams.jpg?raw=1)\n",
        "\n",
        "###3.2.2 Continuous Bag of Words\n",
        "\n",
        "This model takes thes opposite approach from the skip-gram model in that it tries to predict a center word based on the neighboring words. In the case of the CBOW model, we input the context words within the window (such as “the”, “Porsche”, “drove”) and aim to predict the target or center word “speedy” (the input to the prediction pipeline is reversed as compared to the SkipGram model).\n",
        "\n",
        "A graphical depiction of the input to output prediction pipeline for both variants of the Word2vec model is attached. The graphical depiction will help crystallize the difference between SkipGrams and Continuous Bag of Words.\n",
        "\n",
        "![alt text](https://www.dropbox.com/s/k3ddmbtd52wq2li/Image%203%20-%20CBOW%20Model.jpg?raw=1)\n",
        "\n",
        "###3.2.3 Notes on Word Embeddings:\n",
        "\n",
        "1) Word2Vec is useful for topic-modeling, because each word vector contains information about related context words.\n",
        "\n",
        "2) Word2Vec can result in really large and complex vectorizations. In fact, you need Deep Neural Networks to train your Word2Vec models from scratch, but we can use helpful pretrained embeddings to do really cool things!\n",
        "Let's take a look at how to work with these word vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbCusle-77J4"
      },
      "source": [
        "## 3.3 Spacy word embeddings\n",
        "#### `spacy` provides pretrained Word2Vec models.\n",
        "\n",
        "We will use their `en_core_web_md` model, which has 300-dimensional word embeddings for 20,000 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-08acf3f0c46d7591",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "scrolled": true,
        "id": "zfaoM_g7dRAU"
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(\"bananas\")\n",
        "\n",
        "# Get the vector for the token \"bananas\"\n",
        "bananas_vector = doc.vector\n",
        "\n",
        "# print the dimensionality of the vector embedding\n",
        "print(len(bananas_vector))\n",
        "print(bananas_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple = nlp('apple')\n",
        "banana = nlp('banana')\n",
        "gravity = nlp('gravity')\n",
        "\n",
        "print(f'Apple -> Banana similarity: {apple.similarity(banana)}')\n",
        "print(f'Apple -> Gravity similarity: {apple.similarity(gravity)}')\n",
        "print(f'Banana -> Gravity similarity: {banana.similarity(gravity)}')"
      ],
      "metadata": {
        "id": "XHQ4uUf4WI1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD1qE_vCHWuG"
      },
      "source": [
        "### Spacy Document vectors\n",
        "\n",
        "From spacy we can get word2vec embeddings for each word in a document<br>\n",
        "We can create a document vector by averaging all the word vectors in that doc<br>\n",
        "We can calculate the similarity between documents now!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vkq3tlyL2Xp"
      },
      "source": [
        "doc1 = nlp(\"On a hot summer day, it's good to drink water.\")\n",
        "doc2 = nlp(\"There are lots of cold drinks in the refrigerator.\")\n",
        "doc3 = nlp(\"cosine distance measures similarity\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM5RbtPfL38t"
      },
      "source": [
        "len(doc1.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz5eGGl5MKxx"
      },
      "source": [
        "type(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4a00f1280d6a3a32",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "CXNDY_VadRAV"
      },
      "source": [
        "# Get the similarity of doc1 and doc2\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plyigAgONqJ5"
      },
      "source": [
        "doc3.similarity(doc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbXwpFEvcdQG"
      },
      "source": [
        "doc3.similarity(doc1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBwRFUIhzH79"
      },
      "source": [
        "## Challenge: can we make a \"thesaurus\"?<br>\n",
        "Given a query word, do you think we could find good synonyms  <br>\n",
        "by doing a nearest neighbor search on the `spacy` word embeddings?<br>\n",
        "Let's give it a try!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93PNwg0ODA5k"
      },
      "source": [
        "First, get the embeddings for the vocabulary words and make a document-term matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKtrMIBTe4Ja"
      },
      "source": [
        "%%time\n",
        "vocab = list(nlp.vocab.strings)\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhvStTRcYt70"
      },
      "source": [
        "# eliminate duplicates due to mixing upper and lower case\n",
        "vocab =[word.lower() for word in vocab]\n",
        "vocab = list(set(lower_vocab))\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYsIXtUlZobh"
      },
      "source": [
        "%%time\n",
        "word_vectors = [nlp.vocab.get_vector(word) for word in vocab]\n",
        "word_vectors = np.asarray(word_vectors)\n",
        "print(word_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJuBDt0PEh3G"
      },
      "source": [
        "Fit a nearest-neighbors model to the document-term matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3pAOhmto8W"
      },
      "source": [
        "%%time\n",
        "nn = NearestNeighbors(n_neighbors=10, algorithm='auto')\n",
        "\n",
        "# fit to the document-term matrix of all the vocabulary words\n",
        "nn.fit(word_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdrcAOcfChy0"
      },
      "source": [
        "Choose a query word, for which you want to find synonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pBxmi4O5rhp"
      },
      "source": [
        "query_word = 'marvelous'\n",
        "query_vector = nlp.vocab.get_vector(query_word)[None,:]\n",
        "query_vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRbtlDz0EI6Q"
      },
      "source": [
        "Find the nearest neighbors to the query, from spacy's vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plA09PgS68Sc"
      },
      "source": [
        "neigh_dist, neigh_index = nn.kneighbors(query_vector, n_neighbors=20)\n",
        "[vocab[int(index)] for index in neigh_index[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JftffA2DK2Y"
      },
      "source": [
        "## Can we visualize word vectors?\n",
        "\n",
        "300 dimensional word vectors are difficult (impossible) to visualize! <br>\n",
        "However, we can use Principal Components Analysis (PCA) to reduce their dimensionality. <br>\n",
        "\n",
        "PCA is one of the most important techniques in machine learning, and indeed in all of mathematics! PCA transforms the feature vectors of a data set to their \"Principal Components\", which are ranked in order of their importance in describing the data set. By eliminating the less important Principal Components, we can reduce the dimensionality of the data set, at the cost of losing some information.\n",
        "\n",
        "If you're interested to explore this topic in greater depth, here are a couple of references:\n",
        "- One of the best brief introductions to the mathematics of PCA is [Daniela Witten's tweetstorm](https://twitter.com/womeninstat/status/1285610321747611653?lang=en)\n",
        "- For a more in-depth, but still understandable exposition of PCA, watch the Statquest video [Principal Component Analysis (PCA), Step-by-Step](https://youtu.be/FgakZw6K1QQ) from Josh Starmer.\n",
        "\n",
        "Now let's use PCA to reduce our 300-dimensional word vectors down to two dimensions, so that we can visualize them in a plane. Of course, in the process we lose a lot of information, but these 2D word vectors still pack a surprising amount of information!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c54d3921b438215d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "WNGfmJ13dRAV"
      },
      "source": [
        "# import the PCA module from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def get_word_vectors(words):\n",
        "    # converts a list of words into their word vectors\n",
        "    return [nlp(word).vector for word in words]\n",
        "\n",
        "words = ['machine learning', 'man', 'woman',  'king', 'queen',\n",
        "         'artificial intelligence', 'nurse', 'doctor',\n",
        "         'data', 'science', 'concrete', 'wood',  'marble',\n",
        "         'design', 'color', 'font']\n",
        "\n",
        "\n",
        "word_vectors = get_word_vectors(words)\n",
        "\n",
        "\n",
        "# intialise pca model and tell it to project data down onto 2 dimensions\n",
        "\n",
        "# fit the pca model to our 300D data, this will work out which is the best\n",
        "# way to project the data down that will best maintain the relative distances\n",
        "# between data points. It will store these intructioons on how to transform the data.\n",
        "\n",
        "# Tell our (fitted) pca model to transform our 300D data down onto 2D using the\n",
        "# instructions it learnt during the fit phase.\n",
        "\n",
        "# let's look at our new 2D word vectors\n",
        "\n",
        "# intialise pca model and tell it to project data down onto 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# fit the pca model to our 300D data, this will work out which is the best\n",
        "# way to project the data down that will best maintain the relative distances\n",
        "# between data points. It will store these intructioons on how to transform the data.\n",
        "pca.fit(word_vectors)\n",
        "\n",
        "# Tell our (fitted) pca model to transform our 300D data down onto 2D using the\n",
        "# instructions it learnt during the fit phase.\n",
        "word_vecs_2d = pca.transform(word_vectors)\n",
        "\n",
        "# let's look at our new 2D word vectors\n",
        "word_vecs_2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjxAfKfodRAV"
      },
      "source": [
        "# create a nice big plot\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "# plot the scatter plot of where the words will be\n",
        "plt.scatter(word_vecs_2d[:,0], word_vecs_2d[:,1])\n",
        "\n",
        "# for each word and coordinate pair: draw the text on the plot\n",
        "for word, coord in zip(words, word_vecs_2d):\n",
        "    x, y = coord\n",
        "    plt.text(x, y, word, size= 15)\n",
        "\n",
        "# show the plot\n",
        "plt.grid();\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruZeW9_ZDACQ"
      },
      "source": [
        "### Spend a few minutes exploring the magic of word vectors!\n",
        "Create your own `words` list and run it through the code in the above two cells. <br>\n",
        "Perhaps include clusters of words that have similar meanings or that might <br>reveal implicit gender bias or some other kind of bias. What did you find out?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uAb_h7ldRAW"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will extract word embeddings from documents using Spacy's pre-trained model in the upcoming module project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUWsVDc4dRAW"
      },
      "source": [
        "## King - Man + Woman = Queen\n",
        "\n",
        "Check out [**The amazing power of word vectors**](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/), which explores the above equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEkjvdD5dRAW"
      },
      "source": [
        "# Sources\n",
        "\n",
        "* Spacy 101 - https://course.spacy.io\n",
        "* NLTK Book - https://www.nltk.org/book/\n",
        "* An Introduction to Information Retrieval - https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf"
      ]
    }
  ]
}